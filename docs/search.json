[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Flywheels and Public AI",
    "section": "",
    "text": "Preface\nThis is a “mini-book” that discusses “public AI flywheels”: software meant to enable people to opt-in to contribute data towards “public AI” causes. The goal of this book is to support efforts build a transparent, people-centric data collection ecosystem that supports the evaluation and training of public-benefit AI models. If successful, public AI flywheels can create valuable data that materially improves public AI evaluation, research and development. If very successful, these flywheels might also play a role in solving thorny problems around the economics of information in a post-AI age.\nMore frankly, this is way to organize some design notes, practical documentation that’s out of scope for a single example projec’s repo, and longer abstract writing on the topic.\nThis document is organized as such:\n\nIn “Part 1: Concepts”, we explore the motivation and design space of public AI data flywheels.\nIn “Part 2: A Case Study”, we discuss one particular implementation of a Minimum Viable Product (MVP) opt-in flywheel meant to accompany a “public AI interface” (hosted interface software that hits various endpoints for “public AI models”) that uses a “serverless” app + Git backend approach\n\nThis MVP focuses on collecting two high-signal data types: exports of “good chats” and “fail chats.” This data provides immediate value for model evaluation and, at scale, can be used for fine-tuning. Importantly, collecting a list of good and bad chats is also immediately fun, so contributors can get some value before we reach a threshold of data volume needed to construct a full benchmark or dataset. We expect key ideas discussed in this doc, and concretized in this project, to generalize to other data types.\nWe also provide details on how a data retention policy for a concrete Public AI Data Flywheel might work, and more generally discuss the role of the data strategy for a “full stack” public AI application: from model endpoints to OpenWebUI interface to flywheel platform.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01a_intro.html",
    "href": "01a_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is a data flywheel?\nWhat is a data flywheel? Nvidia gives us this definition: “A data flywheel is a feedback loop where data collected from interactions or processes is used to continuously refine AI models.” 1\nIn general, a “data flywheel” is a system or set of systems that capture and/or incentivize data. A “flywheel” generally differs from a more general data collection system because the flywheel is embedded into some kind of application (as opposed to e.g. “standalone” data labeling tasks). So, if I just post a Google form to the Internet and say, “Hey, feel free to use this form to send me data!”, that’s just a form – not a “flywheel”.\nGenerally, most data collection systems lean more towards utilizing either\nHistorically, flywheels tend to imply a passive approach to data collection, but this is not necessarily a requirement. (More on this in a Chapter 3).",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#what-is-a-data-flywheel",
    "href": "01a_intro.html#what-is-a-data-flywheel",
    "title": "1  Introduction",
    "section": "",
    "text": "“sensor-style collection” (passive, instruments like cameras, microphones, or logging software, all of which lack an active “submit data” step) or\n“form-style collection” (active, requiring somebody to click “submit”).",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#what-is-a-public-ai-data-flywheel",
    "href": "01a_intro.html#what-is-a-public-ai-data-flywheel",
    "title": "1  Introduction",
    "section": "1.2 What is a public AI data flywheel?",
    "text": "1.2 What is a public AI data flywheel?\nFirst, what is “public AI”? The public AI network gives us this definition in a whitepaper from (Jackson et al. 2024): AI with\n\n“Public Access – Certain capabilities are so important for participation in public life that access to them should be universal. Public AI provides affordable access to these tools so that everyone can realize their potential.” “Public Accountability – Public AI earns trust by ensuring ultimate control of development rests with the public, giving everyone a chance to participate in shaping the future.” “Permanent Public Goods – Public AI is funded and operated in a way to maintain the public goods it produces permanently, enabling innovators to safely build on a firm foundation.”\n\nFor more on the public AI concept, see also Mozilla’s work in this space and several workshop papers and preprints (from RegML 2023 at NeurIPS, CodeML 2025 at ICML, a recent workshop on Canadian Internet Policy).\nOur focus in this mini-book is building “public AI” flywheels. To summarize heavily – if we try to achieve all the principles laid out in the above work that tries to define “public AI” (and we should try!), we will face some unique challenges in the implementation of data flywheels.\nIn building public AI data flywheels, we are trying to create a feedback loop to improve AI by creating and collecting high-quality data (more on this in Chapter 2). However, the public AI principles mean that we likely want to start from a position of very high accessibility and very high accountability relative to other technology organizations and products. This means we need to provide an accessible explanation of exactly what happens to any data a user creates and give people real agency over the shape of the data pipeline. Ideally, public AI builders should also endeavor to make as many components of our stack as close as possible to public goods, which creates challenges around sustaining effort and funding.\nOf course, it’s worth noting that some particular public could deliberate and make a collective decision that they prefer a more “traditional approach” to data flywheels. Very concretely, we could imagine a state conducting a referendum, and asking the public if they’d like a “public AI” product that follows industry standard practices around data (sacrifing some degree of accessibility and/or accountability for other benefits).\nIn this mini-book, we are taking the stance that it’s best to start from a position of leaning heavily towards a highly accessible and accountable flywheel. We start by minimizing usage and retention of data; data that is used in the flywheel to train AI should be provided via an opt-in by highly informed users.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#core-principles",
    "href": "01a_intro.html#core-principles",
    "title": "1  Introduction",
    "section": "1.3 Core Principles",
    "text": "1.3 Core Principles\nWe can translate the core principles of public AI to the data flywheel domain and arrive at roughly four requirements:\n\nTransparency for informed consent: Users must be fully informed about the models at play, the organizations who are building models, and the ramifications of any contributions to the flyhwheel. Ideally, users will also be informed about the training data underlying the models they use. A detailed FAQ and some kind of consent module (ideally going above and beyond standard Terms of Service2) are required before any data is shared. To some extent, maximally informed consent will require the active expenditure of resoures to improve the public’s AI literary (i.e. we need to build AI literacy focused systems and perhaps even pay people for their attention). We need systems that really do inform people. Luckily, that’s something it seems like AI can help with!\n\n\nData Rights: A public AI data flywheel should empower users with control over their data, mirroring GDPR principles and similar regulations (this is also practically important for compliance). This includes the right to access ($Art. 15$), rectify ($Art. 16$), erase ($Art. 17$), and port data ($Art. 20$). One exemplar project we might look to for inspiration around the implementation of data rights and legal terms is Mozilla’s Common Voice (Ardila et al. 2019).\n\nWe note that data rights conflict with a “fully open” ethos; we will attempt to mitigate these tensions to the best extent possible.\nWe also note that public AI faces some unique challenges with cross-jurisdiction compliance; we discuss this at a high-level later on in ?sec-ec.\n\nBalancing reputation and pseudonymity: To the extent possible, we believe it is valuable to offer people the ability to contribute data with some kind “real account” attached, so people can earn credit and reputation. But this must be balanced with the benefits of also enabling pseudonymity or even anonymity contribution (#todo cite CDSC work on anon contributions).\n\nIn our MVP (discussed in the next chapter) an OpenWebUI or HuggingFace account is required to make contributions, but users can choose to use a pseudonym (not unique; can for instance be “anonymous”). A hashed user id will be stored for internal purposes, but any public data releases will only use the pseudonym.\n\nPurpose Limitation & Licensing: Users should be able to specify their preferences for how their data is used (e.g., for public display? for evaluation? for future model training?). This can be captured using (new) IETF AI Use Preferences and Creative Commons Preference Signals, or other approaches that emerge. We will discuss below how this might extend to other preference signal proposals and/or technical approaches to gating data.\n\nThis is critical for answering a likely FAQ around public AI data – if you succeed in creating actually useful training data or new benchmarks, won’t private labs just immediately use that data as well?\n\n\n\n\n\n\nArdila, Rosana, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. 2019. “Common Voice: A Massively-Multilingual Speech Corpus.” arXiv Preprint arXiv:1912.06670.\n\n\nJackson, Brandon, B Cavello, Flynn Devine, Nick Garcia, Samuel J. Klein, Alex Krasodomski, Joshua Tan, and Eleanor Tursman. 2024. “Public AI: Infrastructure for the Common Good.” Public AI Network. https://doi.org/10.5281/zenodo.13914560.\n\n\nRakova, Bogdana, Renee Shelby, and Megan Ma. 2023. “Terms-We-Serve-with: Five Dimensions for Anticipating and Repairing Algorithmic Harm.” Big Data & Society 10 (2): 20539517231211553.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#footnotes",
    "href": "01a_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Others have also written on data flywheels (see e.g. a number of helpful blogs from Liu, Roche and Sasson, and Del Balso.↩︎\nSee e.g. Terms we serve with. (Rakova, Shelby, and Ma 2023)↩︎",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01b_why.html",
    "href": "01b_why.html",
    "title": "2  Why collect data?",
    "section": "",
    "text": "2.1 An overly detailed accounting of all the ways we might generate LLM pre-training data\nSpeaking at very low-level, LLM pre-training data can come from any sensor or form that creates digital records that contain sequences of tokens. However, we generally don’t want any old tokens – we want tokens that contain signals about the world and about people, and that have been organized (typically by people) in a way that captures structure. In pre-training, it seems we can get away with mixing together many differents types of structure. For post-training, we may want specific structure (e.g. data produced by people following specific instructions).\nWe might further try to describe human-generated data in a very general fashion by saying: data is created when a person does something that leaves a digital trace: typing, speaking into a mic, using some kind of alternate controller, etc. They might also operate a camera or other sensing instrument that captures signals from the world. We also sometimes may want to use truly “sensor-only data” (e.g., seismic readings), though those sensors are built, placed, funded, and so on by humans.\nAfter typing (or other input), they might use a terminal or GUI to send their inputs into some data structure – by committing code, editing a wiki, responding on a forum, and so on. Often, the person creating a record has a goal and/or a task they want to complete. This might be: ask a question, teach or correct something, build software, file a bug, summarize a meeting, translate a passage, or simply react to some information object (like/flag/skip). Critically, in practice, many high value sources of data also have some upstream social structure and corresponding incentives – institutions, communities, etc. that create meaningful incentives for people to produce records that are accurate, insightful, and so on. #todo cite key works about value of social media data, scientific data, etc.\nIn other words, institutions and communities create incentives so that as people type (or otherwise digitize information), they don’t just produce random sequences or the same common sequences repeatedly (or we might have an Internet of web pages that all say “I like good food”; don’t we all…)\nMoving to a more high-level overview, we might begin categorize LLM training data:\nSome specific tasks that might create especially useful data include:\nOf course, a key data for many AI systems is “implict feedback”: clicks, dwell time, scroll/hover, skips/abandonment. This data is typically collected via a “sensor” (logging software), not something users actively contribute through a form.\nhttps://arxiv.org/abs/1712.00409 https://papers.neurips.cc/paper_files/paper/2022/file/7b75da9b61eda40fa35453ee5d077df6-Paper-Conference.pdf https://www.cs.ubc.ca/~hutter/earg/papers07/00585893.pdf\nhttps://arxiv.org/abs/1812.11118\n(article?){Belkin_2019, title={Reconciling modern machine-learning practice and the classical bias–variance trade-off}, volume={116}, ISSN={1091-6490}, url={http://dx.doi.org/10.1073/pnas.1903070116}, DOI={10.1073/pnas.1903070116}, number={32}, journal={Proceedings of the National Academy of Sciences}, publisher={Proceedings of the National Academy of Sciences}, author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik}, year={2019}, month=jul, pages={15849–15854} }",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why collect data?</span>"
    ]
  },
  {
    "objectID": "01b_why.html#an-overly-detailed-accounting-of-all-the-ways-we-might-generate-llm-pre-training-data",
    "href": "01b_why.html#an-overly-detailed-accounting-of-all-the-ways-we-might-generate-llm-pre-training-data",
    "title": "2  Why collect data?",
    "section": "",
    "text": "Human-authored natural language: blogs, books, encyclopedias, news, forums, Q&A, transcripts (talks, meetings, podcasts), documentation, and manuals.\n\nAnd now, some non-human-authored natural language (synthetic versions of any of the above).\n\nCode: source files, perhaps with licenses and provenance, issue threads, commit messages.\nSemi-structured text: tables, markup, configs (HTML/Markdown/LaTeX/YAML/JSON) that carry schema and relationships.\nMultimodal pairs (for VLM/ASR pretraining): image+text, audio+text, video+text, and associated captions/alignment.\n\nHere, the pairing is a critical characteristic that makes this data unique. This implies somebody has looked at the each item in the pair and confirmed a connection (though paired data can be produced in an automated fashion).\n\nMetadata about data: records that describes characteristics of other records. language, domain/topic tags, timestamps, links, authorship/attribution, license, AI preference signals.\n\nQuality signals: dedup scores, perplexity filters, toxicity/PII flags, heuristic or model-based ratings—used to weight or exclude.\n\n\n\n\nAsking a model a question and marking the response “good” or “fail”, optionally with a short note about why.\nCorrections/edits: rewriting a wrong answer; adding a missing citation; supplying a step-by-step solution.\nPairwise preferences: “A is better than B because …” (useful for preference learning/DPO).\nStar ratings / rubrics: numeric or categorical grades on axes like factuality, helpfulness, tone, safety.\nTagging according t os ome taxonomy: topic (“tax law”), language (“id-ID”), difficulty (“HS”), license (CC-BY-SA), and AI preference signals.\nSynthetic tasks: user-written prompts + ideal references (gold answers, test cases, counterexamples).\nMultimodal: an image with a caption; an audio clip with a transcript; a diagram with labeled parts.\nProgrammatic contributions: code snippets with docstrings/tests; minimal reproductions of a bug.\n“Negative” structure: anti-patterns, jailbreak attempts, hallucination catalogs.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why collect data?</span>"
    ]
  },
  {
    "objectID": "01b_why.html#footnotes",
    "href": "01b_why.html#footnotes",
    "title": "2  Why collect data?",
    "section": "",
    "text": "A bayesian might say: data is evidence that updates a prior into a posterior via Bayes’ rule; the “goodness” of a dataset is how much information (likelihood ratio / bits of surprise) it carries about the hypotheses we actually care about. A frequentist might say: data are samples from some process; more (and more representative) samples tighten confidence intervals and reduce estimator variance (roughly with \\(1/\\sqrt{n}\\)), so sampling design and coverage matter as much as sheer volume.↩︎",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why collect data?</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html",
    "href": "01c_pipeworks.html",
    "title": "3  A Democratic Data Pipeworks",
    "section": "",
    "text": "3.1 How does data move from people to AI models — and where can we insert governance levers?\nThis is a summary of a longer Data Leverages Newsletter post.\nTo further motivate the idea of data contribution with public AI principles, it’s worth a brief discussion of what the overall “data pipeworks” of the AI industry looks like from a zoomed out view.\nKey takeaways",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#how-does-data-move-from-people-to-ai-models-and-where-can-we-insert-governance-levers",
    "href": "01c_pipeworks.html#how-does-data-move-from-people-to-ai-models-and-where-can-we-insert-governance-levers",
    "title": "3  A Democratic Data Pipeworks",
    "section": "",
    "text": "Modern AI can be understood as a five-stage pipeworks: (1) Knowledge & Values -&gt; (2) Records -&gt; (3) Datasets -&gt; (4) Models -&gt; (5) Deployed Systems.\nTreating AI as a cybernetic system puts feedback and control at the center. Contributors can steer outcomes by shaping data flow (more on the next chapter).\nHuman factors dominate AI capabilities because they shape what gets recorded upstream. Interfaces, sensors, and incentives are therefore core AI R&D.\n\nsome trends may shift this – RL in real life, #todo cite experiental learning\n\nProperties of data create collective action problems (social dilemmas) that require markets, coalitions, and policy to fix.\nFor public AI flywheels, thinking in terms of data piepworks reveals “insertion points” to add transparency, consent, rights, and preference signals so democratic inputs actually move the system.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#why-a-pipeworks-view",
    "href": "01c_pipeworks.html#why-a-pipeworks-view",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.2 Why a “pipeworks” view?",
    "text": "3.2 Why a “pipeworks” view?\nMost technical AI work zooms in on a clean optimization problem. But questions about who benefits, who participates, and how AI affects society live upstream and downstream of that problem. The Data Pipeworks zooms out. It describes the end-to-end flow by which human activity becomes records, then datasets, then models embedded in systems that act on the world—and thereby change the future data we can collect. That circularity is the opening for governance.\nThis view pairs naturally with cybernetics/control: identify system state, actuators, sensors, and feedback loops; then decide which loops to strengthen or dampen.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#five-stages-of-data",
    "href": "01c_pipeworks.html#five-stages-of-data",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.3 Five stages of data",
    "text": "3.3 Five stages of data\n\nKnowledge & Values (Reality Signal): Humans (and the physical world) generate the latent “signal” AI tries to model (facts, preferences, norms). We don’t presume computability; we note its existence to emphasize sampling implications.\nRecords (Sampling Step): Interfaces and sensors transform activity into structured records (forms, clicks, edits, uploads, buttons, cameras, microphones). Design choices here shape what becomes legible to AI. Key idea: everything either leans “sensor” or “form”.\nDatasets (Filtering & Aggregation): Organizations filter, label, merge, and license records under social, economic, and legal constraints. This determines coverage, bias, and what’s even available to learn from.\nModels (Compression): Learning compresses datasets into input–output mappings. Model choices are path-dependent on Stages 2–3; data defines the feasible hypothesis space.\nDeployed Systems (Actuation): Models are embedded in products, workflows, or infrastructure, producing value and externalities. Deployment feeds back by altering incentives and future record creation.\n\nDesign note: small, well-placed interventions upstream can dominate large downstream tweaks.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#why-this-matters-for-governance-and-alignment",
    "href": "01c_pipeworks.html#why-this-matters-for-governance-and-alignment",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.4 Why this matters for governance and alignment",
    "text": "3.4 Why this matters for governance and alignment\n\nHuman factors are primary. The distributions the AI field is optimizing over are created, not discovered. Interfaces, defaults, prompts, consent flows, and incentives shape the topology of AI work.\nSocial dilemmas are inevitable. Contributing high-quality records to a shared system is a collective action problem (free-riding, failure to reach critical mass). Today’s “dictator solution” (opaque scraping) collapses when people gain data agency.\nData leverage (next chapter) is the steering wheel. Individuals and groups can alter records, licenses, and access. This allows people to steer model behavior by modulating data flow rather than model internals.\nPluralism becomes measurable. Tracing contributions lets us quantify relative weight of individuals and communities, enabling pluralistic governance and new not",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#where-to-place-the-levers-for-public-ai-flywheels",
    "href": "01c_pipeworks.html#where-to-place-the-levers-for-public-ai-flywheels",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.5 Where to place the levers (for public AI flywheels)",
    "text": "3.5 Where to place the levers (for public AI flywheels)\n\nStage 1 to 2 (Knowledge to Records): invest in interfaces and sensors with informed consent; design contribution prompts and micro-tasks; support pseudonymity and reputation choices. Aim to raise signal quality and widen participation.\nStage 2 to 3 (Records to Datasets): attach licenses and AI preference signals per record; validate, de-duplicate, and redact PII; publish partitioned releases. Make rights legible and keep high-trust, high-reuse bundles.\nStage 3 to 4 (Datasets to Models): enable data markets and coalitions, attribution, and sampling weights; build evaluation sets tied to provenance. Align training with community intent and enable bargaining.\nStage 4 to 5 (Models to Systems): publish transparent deployment notes, opt-outs, and model cards tied to data buckets. Surface externalities and set expectations for use.\nStage 5 to 1 (Feedback loop): close the loop with flywheel UX. Leaderboards, grants, bounties, governance hooks (votes, preferences) to sustain contributions and invite further steering.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#implications-for-research-and-practice",
    "href": "01c_pipeworks.html#implications-for-research-and-practice",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.6 Implications for research and practice",
    "text": "3.6 Implications for research and practice\nBuilding flywheels are part of broader agenda to enable a data pipeworks. More in the next chapter on how data contribution through flywheels (including licensed or user-restricted contribution) interplays with data protection, data strikes, markets, etc.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#a-compact-mental-model",
    "href": "01c_pipeworks.html#a-compact-mental-model",
    "title": "3  A Democratic Data Pipeworks",
    "section": "3.7 A compact mental model",
    "text": "3.7 A compact mental model\n\nSensors and interfaces decide what counts.\nFilters and markets decide what persists.\nCompression decides what generalizes.\nDeployment decides what changes next.\nGovernance decides who gets to steer.\n\nPublic AI flywheels turn that loop into a participatory control system: contributors see consequences, express preferences, and are (hopefully) rewarded for adding high-signal records.\n#todo re-add cites from the original post!\nhttps://www.annualreviews.org/content/journals/10.1146/annurev.soc.24.1.183\nhttps://en.wikipedia.org/wiki/Cybernetics\nhttps://www.vox.com/future-perfect/23787024/power-progress-book-ai-history-future-economy-daron-acemoglu-simon-johnson\nhttps://probml.github.io/pml-book/book1.html\nhttps://journals.openedition.org/cybergeo/1035?lang=en\nhttps://dl.acm.org/doi/abs/10.1145/3531146.3533158\nhttps://eckhartarnold.de/papers/2014_Social_Simulations/Whats_wrong_with_social_simulations.html\nhttps://www.anthropic.com/news/influence-functions\nhttps://users.ssc.wisc.edu/~oliver/PROTESTS/ArticleCopies/OliverMarwellCritMassI.pdf\nhttps://raulcastrofernandez.com/papers/data_station_paper-11.pdf",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Democratic Data Pipeworks</span>"
    ]
  },
  {
    "objectID": "01d_leverage.html",
    "href": "01d_leverage.html",
    "title": "4  Flywheels and Bargaining Power",
    "section": "",
    "text": "4.1 How can a public flywheel give people real power over AI systems?\nBased on Chapter 2, we can arrive at a very obvious argument for a data flywheel: the flyhweel will produce data, and that data will make AI better!\nBut this isn’t the only benefit of building flywheels in a “public AI” manner. Doing so can also enhance the amount of agency that people have over data flow, and make “voting with data” possible such that the public has more power to govern and align AI systems.\nIn short, AI is somewhat unique relative to other technologies, because of its data dependence. Data comes from people. The fact that this powerful technology has a dependency on people from around the world means that AI has a natural “governance lever”.\nSetting up a public AI data flywheel is thus important not only to improve AI capabilites; success of public AI data flywheels can collectively help to solve some (but not all!) of the thorny governance and alignment challenges that AI poses by fundamentally changing the data pipeworks of AI.\nYou can read about data leverage via this newsletter or even via this dissertation. For a short summary, of “voting with data to improve alignment”, check out this post: Plural AI Data Alignment.\nIt’s worth pulling out two distinct ways that a flywheel can interact with AI and governance:",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Bargaining Power</span>"
    ]
  },
  {
    "objectID": "01d_leverage.html#how-can-a-public-flywheel-give-people-real-power-over-ai-systems",
    "href": "01d_leverage.html#how-can-a-public-flywheel-give-people-real-power-over-ai-systems",
    "title": "4  Flywheels and Bargaining Power",
    "section": "",
    "text": "A flywheel with no attempt to capture contributor intent or provide data rights may still serve to increase available data, either in fully public repos or in databases accessible by public AI labs. This outcome could still make public models a bit better and help to keep public labs competitive at the margins, but it would not change the bargaining relationship between contributors and model builders.\nA well-governed flywheel that effectively manages the tension between opt-in and friction/ease-of-use can seriously reshape the broader data pipeworks/ecosystem/economy. Ideally this flywheel would also capture provenance, per-item licensing, and per-item AI-use preference (or even enforceable contracts – “you must pay some organization to use this data”, or “you must follow this policy around openness, safety, alignment, etc”). Such flywheels would turn contributions into units that can be assembled, priced, withheld, or targeted, opening the door to markets and, if necessary, strikes.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Bargaining Power</span>"
    ]
  },
  {
    "objectID": "01d_leverage.html#how-an-opt-in-flywheel-enables-markets",
    "href": "01d_leverage.html#how-an-opt-in-flywheel-enables-markets",
    "title": "4  Flywheels and Bargaining Power",
    "section": "4.2 How an opt-in flywheel enables markets",
    "text": "4.2 How an opt-in flywheel enables markets\nAn opt-in flywheel can create the prerequisites for functioning data markets without turning the project into “just a marketplace.”\nCritically, on day one of the data flywheel, each contribution is a unit with provenance, license, usage preferences, and minimal schema. There is also the immediate possibility to associate contributions with reputations of contributors or collectives. This is close to something that is legible enough to transact on. While the initial goal would be to promote conscious data contribution towards public AI causes, it is possible that some data contributors could also use the legibility and the organizing effects of the flywheel to also sell some data to private actors. Indeed, a model already exists that enable people to make public contributions that benefit public interest actors while still allowing large private organizations to pay for data contractually: Wikimedia Enterprise. Wikimedia data is open to all, but Wikimedia is able to monetize “enterprise-level access”. 1\nAs the data flywheel “spins up”, a community could form around the open data to build leaderboards, scarcity tags (rare language/domain), and quality scores. This would effectively begin to generate price signals. A bounty board (“need 5k labeled failures in X”) would serve to convert demand into targeted supply. An exemplar here would be bounty boards for open source software. While the outputs of such bounty boards are code contributions that become OSS (and thus non-excludable), it’s still possible to have market dynamics emerge.\nCo-ops/unions/intermediaries can represent contributors, negotiate bundle terms, run audits, and set default preferences. The flywheel provides a starting shared ledger and release cadence that markets need. (Again in some cases, the intermediary may need to “move off” the flywheel and transact directly in a market).\nThe key idea here is that it’s possible to enable market activity under two distinct sets of conditons: one in which data is kept open-but-gated-and-restricted (“markets” for bespoke Wikimedia Enterprise style packages) or by using the flywheel as a stepping stone towards a more “property-like” market (people organize using the flywheel community or use preference signals as exemplars, then form a data intermediary to collectively bargain directly with data users).",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Bargaining Power</span>"
    ]
  },
  {
    "objectID": "01d_leverage.html#how-an-opt-in-flywheel-enables-strikes-or-credible-refusals",
    "href": "01d_leverage.html#how-an-opt-in-flywheel-enables-strikes-or-credible-refusals",
    "title": "4  Flywheels and Bargaining Power",
    "section": "4.3 How an opt-in flywheel enables strikes (or credible refusals)",
    "text": "4.3 How an opt-in flywheel enables strikes (or credible refusals)\nA data strike here means a coordinated, temporary withdrawal or constraint on high-signal contributions or releases, or retroactive deletion of data (which in some cases, with legal support, could trigger legally enforced retraining https://cyberscoop.com/ftc-algorithm-disgorgement-ai-regulation/ – though TBA on how this will play out in 2025 onwards).\nWhat makes strikes possible:\n\nVoluntariness is preserved. Because contribution is opt-in, non-participation is a legitimate default.\nRelease control. A waiting-room, processing, release pipeline provides a natural “valve” for cadence changes or strikes.\nShared visibility. Everyone sees dependence on fresh contributions (e.g., evaluation drift). Visibility creates leverage.\n\nThere are many variants of data strikes in a flywheel ecosystem:\n\nQuality freeze. Contributors keep using systems but withhold labeled “good/fail” chats or corrections for a period.\nSelective embargo. A community with scarce data (language/domain) pauses releases or flips new records to “evaluation-only.”\nPreference shift. New contributions change AI-use preferences to deny training unless a stated condition is met (funding, governance, attribution).\nRate limit. Collectives cap monthly volume to force negotiations on price or terms.\n\nWhat a strike cannot do (and shouldn’t promise):\n\nUndo past licenses. Items released under irrevocable terms (e.g., CC0, CC-BY) remain available.\nPrevent copying entirely. Public releases can be mirrored; anti-scraping reduces risk but does not eliminate it.\nGuarantee compliance outside the ecosystem. Preference signals work when counterparties agree to honor them or when law/policy backs them.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Bargaining Power</span>"
    ]
  },
  {
    "objectID": "01d_leverage.html#footnotes",
    "href": "01d_leverage.html#footnotes",
    "title": "4  Flywheels and Bargaining Power",
    "section": "",
    "text": "That said, there is no doubt that for certain types of data, some people will need prevent their data from ending up in any public repositories in order to monetize effectively. The public AI data flywheel is only suitable for certain categories of data (in short: content that could be at home in a peer produced knowledge commons). Other types of data may be managed by complementary markets and sharing approaches.↩︎",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Bargaining Power</span>"
    ]
  },
  {
    "objectID": "01e_options_for_flywheel.html",
    "href": "01e_options_for_flywheel.html",
    "title": "5  Flywheel design space",
    "section": "",
    "text": "5.1 Purpose of this section\nThis section gives more context about the many ways we might built flywheels, and lays out alternative governance paths and a future work (in particular, a focus on futures that involve healthy data markets, data intermediaries, federated learning, etc.)\nWe also discuss why we think an approach that includes a minimal retention frontend + opt-in flyhweel platform can serve as a pragmatic bridge to more advanced approaches. For instance, we can use the patterns and concepts used here to move towards independently governed data co-ops, eventual federated learning, etc.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Flywheel design space</span>"
    ]
  },
  {
    "objectID": "01e_options_for_flywheel.html#more-on-all-the-other-approaches-we-couldve-taken",
    "href": "01e_options_for_flywheel.html#more-on-all-the-other-approaches-we-couldve-taken",
    "title": "5  Flywheel design space",
    "section": "5.2 More on all the other approaches we could’ve taken",
    "text": "5.2 More on all the other approaches we could’ve taken\nFirst, let’s lay out a toy model of data “creation” and “flow” (this will come again Part 2, when we walk through the flow for a real flywheel app).\nIn Chapter 2 we talked about the numerous combinations of sensors, forms, task settings, social structure from institutions, communities, etc. that might exist.\nWe might get:\n\nSimple Signal: Binary feedback (👍/👎), star ratings, or flags\nAnnotated Conversation: Full chat with user corrections, ratings, or notes\nPreference Pair: A/B comparisons between responses\nExamples: User-created prompts and ideal responses\nStructured Feedback: Form-based input (error type, severity, correction)\nMultimodal Bundle: Text + images + voice + metadata\nMore advanced structured data …\n\nFurther, the creation of data might be prompted at several points in time:\n\nProactive: User initiates contribution unprompted (e.g., “Share this chat” button)\nReactive: System prompts based on signals (e.g., after thumbs down or trigger word, ask “What went wrong?”)\nPassive: Automatic collection with prior consent (e.g., telemetry, browser extension)\nScheduled: Regular prompts (e.g., weekly “best conversations” review)\nTask-Based: Specific requests for data types (e.g., “Help us improve math responses”)\n\nThis choice will likely impact the level of “friction” users experience:\n\nZero-Friction: One-click actions with no interruption\nLow-Friction: Modal popup or inline form\nMedium-Friction: Redirect to separate interface\nHigh-Friction: Multi-step process, account creation, or technical skills required\n\nData might also be processed at one or more points in time (In practice, there may be some processing at various steps, but it is important to clarify this to users):\n\nPre-submission: Client-side processing before data leaves user’s device\nOn-submission: Real-time processing during the contribution flow\nPost-submission: Batch processing after data is received\nPre-publication: Review and processing before making data public\nOn-demand: Processing happens when data is accessed/downloaded\n\nLet’s now collapse that and say: a person visits an AI interface (e.g. visits a chatbot product on a website). They sit down to type and query, and then react (take the information and do something with it, follow up, leave positive or negative feedback, etc.). This is our canonical object of interest: a query, response, and optional follow up data (feedback, more queries and responses, etc.).\nThis data must live, for some time, on the user’s device. It must also hit the AI model, which may either be a hosted service or another local device (if e.g. user is running open weights on their own device). It may or may not be stored on the server/system (we’ll use these interchangeably for now to refer to all the devices controlled by the organization running each module) where the interface is hosted. It may or may not be stored by the server/system where the model is hosted. And finally, a flywheel may send that data to a third location.\nThis final data could live in a centralized database (e.g. traditional relational database), a public repository (e.g. GitHub, HuggingFace), totally local, or even in some kind of distributed network (IPFS, BitTorrent).\nFinally, this data might be accessed in a number of ways:\n\nDirect Download: Raw access to complete dataset (with rate limits)\nAPI Access: Programmatic access with authentication and quotas\nStatic Site: Read-only web interface with anti-scraping measures\nGated Access: Application/approval process for researchers\nHybrid Access: Public samples + gated full access, or public metadata + restricted content\nStreaming Access: Real-time feeds for continuous model training",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Flywheel design space</span>"
    ]
  },
  {
    "objectID": "01e_options_for_flywheel.html#some-categories-of-architectural-models",
    "href": "01e_options_for_flywheel.html#some-categories-of-architectural-models",
    "title": "5  Flywheel design space",
    "section": "5.3 Some Categories of Architectural Models",
    "text": "5.3 Some Categories of Architectural Models\nWith all these design choices in mind, it will be useful to describe the general approaches we might take to build a data flywheel.\n\n5.3.1 Standard “PrivateCo” Web App\nAn obvious option is to simply build a hosted “standard” “PrivateCo” / start-up style web app. If Netflix is successful because of its flywheel, why not just build a public AI data flywheel that looks like a private tech company’s product from a technical perspective? Indeed, in some contexts it may make sense to skip building an opt-in flyhweel and simply use the data generated by users directly for training, eval, etc. In this case, there is no “third location” needed; just read data from the existing prod database. While one could argue that the Terms of Service for many existing tech products do make these products “opt in” in some sense, there are also serious downsides to the status quo (see e.g. Fiesler, Lampe, and Bruckman 2016.)\nWhile perhaps some users might prefer even prefer a start-up style model, we believe this would not be a good starting place for a public AI interface. We also believe it’s important to communicate to users how the public AI interface differs from e.g. using ChatGPT, Gemini, or AI overviews via search.\nThis approach doesn’t really constrain how we answer most of the above questions. Under this approach, we can collect all types of signals, mix proactive and reactive data collection, use telemetry freely, process data whenever we want. It’s highly likely that data woud live in centralized database. It’s also likely we would want to follow corporate practices in locking down the final data, which makes this a bad choice for maximizing publicly visible output (put simply: we probably can’t run an AI product that has a prod database that is openly readable by the public.)\n\n\n5.3.2 Git/Wiki Platform\nAnother option to build a “very active flywheel” (that arguably stretches the definition because friction will be very high) is to just deploy a server for git or wiki style peer production.\nNow, we do likely constrain our answers to the above questions:\n\nWhere data lives: Public repository\nWhen prompted: Proactive (user initiates)\nWhen processed: Pre-submission (user does it) + CI/CD validation\nHow accessed: Direct download via Git + web interface\nFriction level: High (technical knowledge required)\nPros: Maximum transparency, built-in versioning, low cost\nCons: Excludes non-technical users, limited data types\nExample Stack: some combo of MediaWiki, GitHub, GitLab, HuggingFace + CI/CD validation\n\nHowever, this approach has large technical barriers to entry and is high friction even for technical users.\n\n\n5.3.3 Web service + Git Platform\nThe option described in Part 2 is to use a Git/Wiki approach, but use a serverless web service (or a more traditional app; doesn’t have to be serverless) with special endpoints that are triggered by users via low friction in-app actions (clicking a special button, entering special command, etc.) that writes to a Wiki / Git repo on the contributor’s behalf. We do enable the possibility that users can save settings that effectively commit data to the source control / wiki system automatically (e.g., “Every day, run an anonymization script on my chat history and then write the output as a new file to a shared, version-controlled server”).\n\nWhere data lives: Public repository\nWhen prompted: Proactive or reactive\nWhen processed: On-submission via serverless function\nHow accessed: Git access + static site generation\nFriction level: Low (automated complexity)\nPros: Transparency + usability, serverless scaling\nCons: Technical issues Cold starts, API rate limits, complex error handling\nExample Stack: Vercel/Netlify + GitHub API + Hugging Face Hub\n\n\n\n5.3.4 Federated Learning Model\nOne radically different approach might involve using federated learning.\n\nWhere data lives: User devices (distributed)\nWhen prompted: Passive with consent\nInformation object: Model gradients or aggregated statistics\nWhen processed: Pre-submission (on-device)\nHow accessed: Only aggregated model updates available\nFriction level: Zero after setup\nPros: Maximum privacy, no data transfer, infinite scale\nCons: Complex implementation, limited debugging, device requirements\n\n\n\n5.3.5 Browser Extension\nWe could implement a flywheel that relies on users downloading a browser extension! This only reflects a data ingestion choice: can be used with various backend choices above.\n\nWhere data lives: Centralized or distributed\nWhen prompted: Proactive or passive\nInformation object: DOM captures, interaction logs, selections\nWhen processed: Depends on backend\nHow accessed: Depends on storage choice\nFriction level: Very low after installation\n\n\n\n5.3.6 P2P Network Model\n\nWhere data lives: Distributed across peer nodes\nWhen prompted: Passive (background sharing)\nInformation object: Torrent-style data chunks\nWhen processed: Pre-submission by contributor + network validation\nHow accessed: P2P client required for full access\nFriction level: Medium (client installation)\nPros: No infrastructure costs, censorship resistant\nCons: Availability issues, complex coordination\nExample Stack: libp2p + BitTorrent protocol + DHT",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Flywheel design space</span>"
    ]
  },
  {
    "objectID": "01e_options_for_flywheel.html#scenario-walkthroughs-a-practical-comparison",
    "href": "01e_options_for_flywheel.html#scenario-walkthroughs-a-practical-comparison",
    "title": "5  Flywheel design space",
    "section": "5.4 Scenario Walkthroughs: A Practical Comparison",
    "text": "5.4 Scenario Walkthroughs: A Practical Comparison\nHere, we walk through two common scenarios and describe what happens (in one sentence) for each of the architectures described above.\n#todo: these could be made crisper to highlight the key differences better (But also be honest about where there are similarities)\n\n5.4.1 Scenario A: User marks a chat as “Good” – when does processing happen?\n\nWeb App: Redirects to platform, PII scrubbed on submission, available via API after review\nGit/Wiki: User removes PII manually, creates PR, instantly visible on merge\nTelemetry: Signal sent, processed in real-time, only visible in aggregates\nHybrid: Signal sent immediately, full chat processed if shared\nServerless+Git: Modal appears, serverless function strips PII, PR created automatically\nFederated: Local processing only, contributes to next model update\nExtension: Captures state, removes PII client-side, sends to chosen backend\nP2P: Processes locally, shares with peers who validate before propagating\n\n\n\n5.4.2 Scenario B: User corrects a factual error\n\nWeb App: Editor interface, toxicity check on submission, published after human review\nGit/Wiki: User edits markdown, CI/CD checks format, visible immediately on merge\nTelemetry: Only captures “error” signal, no correction possible\nHybrid: Error signal triggers correction UI, correction queued for review\nServerless+Git: Inline correction, automated PII/toxicity checks, PR needs approval\nFederated: Correction processed locally, differential privacy applied\nExtension: Highlights error, pre-processes correction, sends to backend\nP2P: Broadcasts correction, network consensus before acceptance\n\n\n\n5.4.3 Scenario C: Accessing the contributed data\n\nWeb App: Researchers apply for API key, public sees samples on static site\nGit/Wiki: Anyone can clone repo, but rate-limited through CDN\nTelemetry: Only aggregated statistics available via public dashboard\nHybrid: Public can see signals dashboard, researchers apply for conversation access\nServerless+Git: Public (or gated) repo with all data, static site with search/filter\nFederated: No direct data access, only model checkpoints released\nExtension: Depends on backend choice, typically follows that model\nP2P: Must run client to access network, can specify data sharing preferences",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Flywheel design space</span>"
    ]
  },
  {
    "objectID": "01e_options_for_flywheel.html#frontier-approaches-data-cooperatives-federated-learning-and-more",
    "href": "01e_options_for_flywheel.html#frontier-approaches-data-cooperatives-federated-learning-and-more",
    "title": "5  Flywheel design space",
    "section": "5.5 Frontier approaches: data cooperatives, federated learning, and more",
    "text": "5.5 Frontier approaches: data cooperatives, federated learning, and more\nIn many cases, users may want to have data governed by community organizations (e.g., organized by domain/region/language) that hold rights and decide release cadence, licensing defaults, and benefit policies.\nPractically, taking a collective/intermediary focused approach has the potential to massively reduce user friction / attention costs (join intermediary once a year; delegate key decision-making. If joining process is good + governance is good, can achieve good outcomes).\nWe note that because our implementation is built on top of open-source software, communities can easily choose to deploy their own OpenWebUI instance and their own data flywheel and effectively operate entirely parallel, self-governed instances. If they also choose to share opt-in data via similar licensing and preference signal approaches, such datasets could be easily merged – but with fine-grained adjustments to precise details (e.g., slight modifications on retention, access, release cadence, content moderation, adn so on.) Of course, data co-ops may choose to use quite different technical stacks. This approach is just one among many.\nIt may be possible to also move from an opt-in data flywheel approach to a federated learning-first approach. Here, model training occurs across user or institutional nodes; only gradients/updates (with privacy tech) are centralized. The dataset remains partitioned or local; central custodian minimized. This approach would:\n\nReduces central data custody and breach surface\nAligns with data-residency and institutional constraints\nEnables “learning from data that can’t leave”\n\nBut has some major downsides / existing barriers:\n\nHarder reproducibility and data auditability\nComplex privacy stack (secure aggregation, DP, client attestation)\nBenchmarking must be redesigned (federated eval)\n\nThis is a bigger leap, but we believe it’s important to begin to think about how the implementation of the Public AI Data Flywheels might support communities wishing to transition towards an FL approach.\nOne rough sketch might look like: * Build the MVP defined in Chapter 2 * Ship license + AI-preference metadata (MVP). * Maintain gated HF releases and public leaderboards/full data access. * Publish provider-payload transparency and link to provider terms (no guarantees). * Process deletions via HF mechanisms when possible; keep our mirrors in sync. * Phase 1 — Co-op pilots * Charter one or two community co-ops; define bylaws, scope, and release cadence. * Spin up many instances of interface + flywheel combos (can fork software directly, or use similar approaches) * Establish a concrete sharing / merging plan * And beyond! * Once several independent data communities, are operated, it might be possible to move from lightweight sharing and merging to more serious federation with technical guarantees. Perhaps this might start with federated evaluation and then move to federated training. Much more to do here, out of scope for this document.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Flywheel design space</span>"
    ]
  },
  {
    "objectID": "01f_ethics_compliance.html",
    "href": "01f_ethics_compliance.html",
    "title": "6  Ethics and Compliance",
    "section": "",
    "text": "6.1 Ethics",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics and Compliance</span>"
    ]
  },
  {
    "objectID": "01f_ethics_compliance.html#ethics",
    "href": "01f_ethics_compliance.html#ethics",
    "title": "6  Ethics and Compliance",
    "section": "",
    "text": "6.1.1 Flywheel-particular challenges\nThere is a large literature on harms from AI and sociotechnical systems more generally. We provide a longer set of references at the end of this section.\nThe top ethics priority for a PAIDF is figuring out informed consent, and balancing consent and friction. One worst case scenario for a a public AI organization is that the flywheel is set up in a way that erodes user trust and ultimately hinders the broader public AI mission.\nWhile designing ethical systems normally involves some degree of multiplicity (there is a rarely a single “most ethical solution” for a given group of people), our overall stance is that informed consent can be achieved by maximizing user information about data use and taking a fundementally opt in approach.\nBeyond consent, a number of other interesting ethics challenges arise. We describe them first, and then discuss the intersection between building an ethical flywheel and a compliant flywheel.\nIn particular, there are three flywheel specific concerns, that primarily stem from the very general nature of modern AI data. First, it is possible that data that is contributed via the flywheel could create serious security concerns (contributing a chat that includes an injection attack). Second, data that is contributed could create privacy concerns (PII and sensitive strings, from email, names to API keys). And third, data that is contributed be seen as expressively harmful or leading to representational harms. That is, some users might produce data that is very offensive to other users. This is likely inevitable in a large enough system, and so public AI flywheel designer must plan with values conflict in mind.\nIn short, when we open up a form to the world, people may enter things (even in good faith) that creates security risks, violates privacy, or violates social norms. We\nThere are also a set of ethical risks that arise from downstream AI systems that we build/improve with flywheel data. While these are not the focus of this mini-book, it is critical to keep them in mind. A non-exhaustive list includes:\n\nAI-driven expressive harms: the system produces content that demeans, stereotypes, or legitimizes abuse against protected groups\nAI-driven representational harms: skewed data makes groups invisible or mischaracterized (e.g., images that underrepresent darker skin tones; code comments that assume a single gender)\nallocative harms: outputs affect access to opportunities or resources (moderation, ranking, credit scores)\nprivacy harms at the model layer (distinct from data layer): re-identification, doxxing, accidental leakage of personal or sensitive data\nsecurity harms (distinct from data layer): prompt injection and data exfiltration via model behavior; poisoning of training or eval sets\nIP and contract harms: misuse of copyrighted or licensed content; violations of platform terms\n\n\n\n6.1.2 Flywheel-specific high level goals\nTo balance these ethical challenges, we might organize our design around high-level goals that often appear in AI regulation and ethical discussions. These might include “purpose limitation” (our flywheel should try to collect only data that is necessary for the stated task – evaluating and improving AI systems) and “proportionality” (we should weigh utility of data collection against the likelihood and severity of harm; to some extent, because the flywheel leans opt-in, some decision-making is delegated to contributors). Considering the more general set of AI harms above, we may also want to specifically acquire or filter data in a way that helps achieve fairness goals.\nTypically, you will see works attempt to classify high-risk data which should be treated differently. Examples include:\n\nfaces, voices, gait, or other biometrics\nimages of minors or contexts involving schools and hospitals (Federal Trade Commission 2013; U.S. Department of Education 1974; U.S. Department of Health and Human Services 2000).\nintimate or medical contexts, support forums, addiction and mental health groups\ngovernment IDs, financial records, geolocation trails, and precise timestamps\ncredential artifacts: API keys, cookies, session tokens, SSH keys, access logs\ncontent from communities with clear norms against scraping or model training\ndatasets whose provenance is unclear or license compatibility is uncertain\n\nA flywheel designer likely wants to avoid collecting this kind of data, but getting 100% precision will be nearly impossible, because some of the most interesting AI outputs (especially failure cases) may involve high-stakes scenarios. A flywheel that completely bans contributions related to cybersecurity or human health risks collecting “bland” data.\n\n\n6.1.3 Levers for solving these ethics challenges\nThe flywheel designer can several avenues for attempting to pre-empt some of the above challenges. In terms of informed consent, this comes down to the implementation of a usable, informative module for consent and the exact UX for opting in and out. In terms of security and privacy, this mainly comes down to implementing filtering/curation at various stages. In terms of values conflict, the designer may employ filter, but also take a normative or sociotechnical approach (leaning on peer production-style talk pages, moderation, community-generated rules, etc.). The designer has the least leverage to directly control downstream model harms, but can have some influence via further training data filtering, helping to document data produced by the flywheel, etc.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics and Compliance</span>"
    ]
  },
  {
    "objectID": "01f_ethics_compliance.html#compliance",
    "href": "01f_ethics_compliance.html#compliance",
    "title": "6  Ethics and Compliance",
    "section": "6.2 Compliance",
    "text": "6.2 Compliance\nIn general, data protection regimes impose responsibilites on anyone operating a platform.\nMost likely, any public AI data flywheel will also be connected some frontend (e.g., hosted OpenWebUI instance) and some backend (model provider). These distinct systems are likely to have their own data-related responsibilites, depending on exactly how they hold or process data.\nExamples of these duties include:\nGDPR - controller / processor concepts -\nCCPA\n\n6.2.1 Risks\nIn terms of compliance risks, some issues may emerge because of contributor mistakes: users may post personal data that evades whatever filtering/curation the designer has implemented. In way, PII, secrets, or identifiers may make it into the flywheel’s data repo. Further, even when users make contributions via pseudonym, unique phrasing or context can deanonymize.Ssalted contributor hashes are still stable identifiers across contributions\nIn general, a major risk with an approach that creates publicly accessible data is the potential for permanence via forks and mirrors. Removed data can persist in external forks, local clones, or third-party mirrors outside this project’s control. Further, while repo history can be rewritten and monthly files reissued, but downstream models may already have trained; unlearning is best-effort and not guaranteed\nAnother issues related to the use of various vendors. Hosting providers (e.g. Vercel and similar services, any caching databases uses, any APIs used) may retain request logs; this is outside the app’s control.\nIn some cases, contributions that create “security-related ethical risks” (e.g. a chat in which an LLM provides instruction for conducting some kind of attack) could also create compliance risks. This creates some continuous burden on maintainers. The same is true of offensive content or privacy violations. Even with consent and public repos, some jurisdictions treat certain content types as sensitive or restricted",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics and Compliance</span>"
    ]
  },
  {
    "objectID": "01f_ethics_compliance.html#further-reading",
    "href": "01f_ethics_compliance.html#further-reading",
    "title": "6  Ethics and Compliance",
    "section": "6.3 Further reading:",
    "text": "6.3 Further reading:\nFirst: works that taxonomize harms (Shelby et al. 2023; Weidinger, Mellor, et al. 2021; Blodgett et al. 2020)\nWorks that discuss expressively harms and representative harms (Shelby et al. 2023; Weidinger, Mellor, et al. 2021; Buolamwini and Gebru 2018; Grother, Ngan, and Hanaoka 2019; Crawford and Paglen 2019; Blodgett et al. 2020).\nOn data that has actual security concerns (contributing a chat that includes an injection attack) (OWASP 2023; Hubinger et al. 2024; Carlini et al. 2024).\nOn PII and sensitive strings (from email, names to API keys) (McCallister, Grance, and Scarfone 2010; European Union 2016; Illinois General Assembly 2008; “Rosenbach v. Six Flags Entertainment Corp.” 2019; Carlini et al. 2019, 2021).\nFurther reading on:\n\npurpose limitation and reversability: (European Union 2016).\nproportionality: weigh utility against the likelihood and severity of harm (ISO/IEC 23894:2023 Information Technology—Artificial Intelligence—Risk Management 2023; NIST 2023).\nrespect for context: treat data according to the social norms of its origin community (Nissenbaum 2004; Jo and Gebru 2020).\ntransparency: explain collection, uses, and the limits of control in clear language (Mitchell et al. 2019; Gebru et al. 2018; Holland et al. 2018).\naccountability: assign owners, metrics, and escalation paths (NIST 2023; European Union 2024).\nfairness and non-discrimination: measure and mitigate disparate impacts (Barocas and Selbst 2016; Selbst et al. 2019; Obermeyer et al. 2019; Bender et al. 2021).\nallocative harms: outputs affect access to opportunities or resources (moderation, ranking, credit-like inferences) (Barocas and Selbst 2016; Obermeyer et al. 2019).\nprivacy harms: re-identification, doxxing, accidental leakage of personal or sensitive data (Sweeney 2000; Narayanan and Shmatikov 2008; Carlini et al. 2021).\nsecurity harms: prompt injection and data exfiltration via model behavior; poisoning of training or eval sets (OWASP 2023; Carlini et al. 2024).\nIP and contract harms: misuse of copyrighted or licensed content; violations of platform terms (U.S. Copyright Office 2024; Creative Commons 2023).\n\nWorks on high-risk data:\n\nfaces, voices, gait, or other biometrics (European Union 2016; Illinois General Assembly 2008).\ngovernment IDs, financial records, geolocation trails, and precise timestamps (McCallister, Grance, and Scarfone 2010).\ncredential artifacts: API keys, cookies, session tokens, SSH keys, access logs (Carlini et al. 2019, 2021).\ncontent from communities with clear norms against scraping or model training (Nissenbaum 2004; Jo and Gebru 2020).\ndatasets whose provenance is unclear or license compatibility is uncertain (Common Crawl 2022; Schuhmann et al. 2022; Creative Commons 2023).\n\n\n\n\n\nBarocas, Solon, and Andrew D. Selbst. 2016. “Big Data’s Disparate Impact.” California Law Review 104 (3): 671–732.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT), 610–23.\n\n\nBlodgett, Su Lin, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. “Language (Technology) Is Power: A Critical Survey of ‘Bias’ in NLP.” In Proceedings of ACL, 5454–76.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Proceedings of the Conference on Fairness, Accountability and Transparency (FAT*), 77–91.\n\n\nCarlini, Nicholas, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramèr. 2024. “Poisoning Web-Scale Training Datasets Is Practical.” https://arxiv.org/abs/2302.10149.\n\n\nCarlini, Nicholas, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. “The Secret Sharer: Measuring Unintended Memorization in Neural Networks.” In Proceedings of USENIX Security Symposium.\n\n\nCarlini, Nicholas, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, et al. 2021. “Extracting Training Data from Large Language Models.” In Proceedings of USENIX Security Symposium.\n\n\nCommon Crawl. 2022. “Common Crawl — Web-Scale Data for Research.” https://commoncrawl.org/.\n\n\nCrawford, Kate, and Trevor Paglen. 2019. “Excavating AI: The Politics of Images in Machine Learning Training Sets.” https://www.excavating.ai/.\n\n\nCreative Commons. 2023. “Understanding CC Licenses and Generative AI.” https://creativecommons.org/2023/08/18/understanding-cc-licenses-and-generative-ai/.\n\n\nEuropean Union. 2016. “General Data Protection Regulation (EU) 2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\n———. 2024. “Artificial Intelligence Act.” https://eur-lex.europa.eu/.\n\n\nFederal Trade Commission. 2013. “Children’s Online Privacy Protection Rule (COPPA) — 16 CFR Part 312.” https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018. “Datasheets for Datasets.” In arXiv:1803.09010.\n\n\nGrother, Patrick, Mei Ngan, and Kayee Hanaoka. 2019. “Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects.” NISTIR 8280. NIST. https://doi.org/10.6028/NIST.IR.8280.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. “The Dataset Nutrition Label: A Framework to Drive Higher Data Quality Standards.” https://arxiv.org/abs/1805.03677.\n\n\nHubinger, Evan, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, et al. 2024. “Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training.” https://arxiv.org/abs/2401.05566.\n\n\nIllinois General Assembly. 2008. “Biometric Information Privacy Act (BIPA), 740 ILCS 14.” https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004.\n\n\nISO/IEC 23894:2023 Information Technology—Artificial Intelligence—Risk Management. 2023. ISO/IEC.\n\n\nJo, Emily, and Timnit Gebru. 2020. “Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning.” In Proceedings of FAccT, 306–16.\n\n\nMcCallister, Erika, Tim Grance, and Karen Scarfone. 2010. “Guide to Protecting the Confidentiality of Personally Identifiable Information (PII).” SP 800-122. NIST.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT), 220–29.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2008. “Robust de-Anonymization of Large Sparse Datasets.” In Proceedings of the IEEE Symposium on Security and Privacy, 111–25.\n\n\nNissenbaum, Helen. 2004. “Privacy as Contextual Integrity.” Washington Law Review 79 (1): 119–57.\n\n\nNIST. 2023. “Artificial Intelligence Risk Management Framework (AI RMF 1.0).” NIST AI 100-1. National Institute of Standards; Technology; https://www.nist.gov/ai.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\n\nOWASP. 2023. “OWASP Top 10 for Large Language Model Applications.” https://owasp.org/www-project-top-10-for-large-language-model-applications/.\n\n\n“Rosenbach v. Six Flags Entertainment Corp.” 2019. 2019 IL 123186, Supreme Court of Illinois.\n\n\nSchuhmann, Christoph, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, et al. 2022. “LAION-5B: An Open Large-Scale Dataset for Training Next CLIP Models.” In Proceedings of NeurIPS Datasets and Benchmarks.\n\n\nSelbst, Andrew D., Danah Boyd, Suresh Venkatasubramanian Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. “Fairness and Abstraction in Sociotechnical Systems.” In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT), 59–68.\n\n\nShelby, Renee, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas, N’Mah Yilla-Akbari, et al. 2023. “Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction.” In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 723–41. AIES ’23. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3600211.3604673.\n\n\nSweeney, Latanya. 2000. “Simple Demographics Often Identify People Uniquely.” Carnegie Mellon University, Data Privacy Working Paper.\n\n\nU.S. Copyright Office. 2024. “Copyright and Artificial Intelligence: Policy Studies and Guidance.” https://copyright.gov/ai/.\n\n\nU.S. Department of Education. 1974. “Family Educational Rights and Privacy Act (FERPA).” https://www2.ed.gov/policy/gen/guid/fpco/ferpa/index.html.\n\n\nU.S. Department of Health and Human Services. 2000. “HIPAA Privacy Rule — 45 CFR Parts 160 and 164.” https://www.hhs.gov/hipaa/for-professionals/privacy/index.html.\n\n\nWeidinger, Laura, John Mellor, et al. 2021. “Ethical and Social Risks of Harm from Language Models.” arXiv Preprint arXiv:2112.04359.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics and Compliance</span>"
    ]
  },
  {
    "objectID": "01z_upstream.html",
    "href": "01z_upstream.html",
    "title": "7  Upstream data and data contribution",
    "section": "",
    "text": "7.0.1 AI builder attribution\nAt a high-level: in each interaction between users and a public AI system, we want to attribute the organization who did the hard work of prepping a model.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Upstream data and data contribution</span>"
    ]
  },
  {
    "objectID": "01z_upstream.html#why-does-upstream-matter",
    "href": "01z_upstream.html#why-does-upstream-matter",
    "title": "7  Upstream data and data contribution",
    "section": "7.1 Why does upstream matter?",
    "text": "7.1 Why does upstream matter?\nTelling users about upstream data is a key part of system-wide transparency. Transparency on both fronts (model builders, data) has the potential to provide further incentive to users to provide data in the first place (because, e.g., they specifically want to support one of the organizations providing models or data).\nThere are a number of other exciting conncetions between data valuation/attribution, collective action in data (algorithmic collective action, data leverage), and flywheels.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Upstream data and data contribution</span>"
    ]
  },
  {
    "objectID": "01z_upstream.html#further-reading",
    "href": "01z_upstream.html#further-reading",
    "title": "7  Upstream data and data contribution",
    "section": "7.2 Further reading:",
    "text": "7.2 Further reading:\n\n\n\n\nLiu, Jiacheng, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, et al. 2025. “OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens.” arXiv Preprint arXiv:2504.07096.",
    "crumbs": [
      "Concepts & Rationale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Upstream data and data contribution</span>"
    ]
  },
  {
    "objectID": "2a_mvp.html",
    "href": "2a_mvp.html",
    "title": "8  The Serverless + Git MVP",
    "section": "",
    "text": "8.1 Overview\nOur initial MVP of the flywheel is a “Serverless + Git Platform” approach. It is meant to be a robust and scalable starting point for the data flywheel that has strict separation between the “open, opt-in data stored in the flywheel repo” and the “user data and logs needed to operate an LLM frontend”.\nThe overall goal is to enable opt-in contributions of data (prompt, output, good/bad, optional metadata) with relatively open licenses (per-item Creative Commons license: default is CC0, CC-BY, CC-BY-SA), state-of-the-art preference signals (using IETF aipref draft spec + CC Preference Signals draft spec; caveat that these are untested) and enforcement (HuggingFace terms of use + Cloudflare anti-scraping). The data is distributed via:\nFor full details, see the separate repo and its readme: #todo fill me in\nFor a short summary of the technical approach, see below bullet points:",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "2a_mvp.html#overview",
    "href": "2a_mvp.html#overview",
    "title": "8  The Serverless + Git MVP",
    "section": "",
    "text": "Hugging Face (gated) — full bundles by “license/usage bucket”; access via request.\nPublic site — leaderboards + full dataset access; Cloudflare WAF/bot controls to block scraping.\n\n\n\n\nFrontend: A Next.js application hosted on Vercel, providing a simple, static interface for contributions and an API endpoint that talks directly to OpenWebUI (idea is that most contribution volume will come directly via OpenWebUI, not a static site).\n\nNo major lock-in here. Can easily be swapped for a lightweight static site, other modern web tech, etc.\n\nAuthentication: User identity is managed via Auth.js (Next-Auth), using Hugging Face (HF) or OpenWebUI accounts. This allows for clear attribution of contributions to a user’s public username (if they choose to).\nData Storage: The single source of truth is a Hugging Face Dataset repository, which functions as a “Git-as-a-database.”\n\nStarting with HF as it is a platform with specific focus on AI datasets.\n\nWaiting room approach: The implemented workflow follows a two-stage “waiting room” pattern to ensure data quality and safety:\nHow contribution works:\n\nA user logs in.\nThe user contributes via OpenWebUI or static site.\n\nUsers choose a label for the chat type (“Good Chat” / “Fail Chat”), attach a Creative Commons license, and attach an an IETF aipref+ Creative Commons Preference Signal to signal preferences around AI use of the contribution.\n\nA pseudo-anonymity system allows users to contribute publicly with their HF or OpenWebUI username, as “anonymous,” or with a custom pseudonym.\nUpon submission, a serverless function writes the contribution not to the final dataset, but as a new, single JSON file in a _waiting_room/ directory within a PRIVATE Hugging Face repository. This operation is fast and avoids write conflicts.\n\nHow processing of the “waiting room” works:\n\nA separate, asynchronous script is run on a regular schedule (e.g., as a daily GitHub Action).\nThis script fetches all pending files from the _waiting_room.\nIt validates each contribution and includes a placeholder for future content moderation and PII checks.\nValidated contributions are batched and appended to a final, organized dataset file in a public-but-gated HuggingFace repo (e.g., data/2025-08.jsonl). Contributions are further bucketed by license/prefs.\nTo ensure atomicity, all file additions (to the final dataset) and deletions (from the waiting room) are performed in a single commit to the Hugging Face Hub. #todo when scaling, need to consider race conditions around the processing!",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "2a_mvp.html#advantages-of-a-serverless-git-approach",
    "href": "2a_mvp.html#advantages-of-a-serverless-git-approach",
    "title": "8  The Serverless + Git MVP",
    "section": "8.2 Advantages of a Serverless + Git approach",
    "text": "8.2 Advantages of a Serverless + Git approach\nA serverless + Git stack keeps the “write path” lightweight for contributors and cheap to operate. Functions spin up on demand and idle to zero, so we can avoid paying for boxes that sit around; the trade-off is cold starts, which are well-documented and can be mitigated with provisioned concurrency when needed.\nOn the “read path,” a static site on a global CDN gives instant distribution and low operational overhead. Pages (e.g., from Cloudflare) can read directly from the Git “source of truth” and serve assets from edge locations by default, which is exactly what we want for a browsable leaderboard and dataset browser.\nAdditionally, using the Hub (Git-backed) as the source of truth buys us a public audit trail and first-class versioning semantics. HF’s dataset repos are literally Git + LFS, with revision pinning via commit/tag/branch; storage is backed by object storage and scales. That maps cleanly to our workflow and makes it easy to diff changes over time. (relevant HF docs: datasets, storage)\nModeration and PII handling are naturally centralized in the processing step. Because we trigger the write as a small file into a staging path and move it during a scheduled job, we can run filters, de-dup, and attach license/pref metadata before publication without asking contributors to learn tooling.\nBasic safety and access controls are pragmatic at the edge. Cloudflare’s newer “AI bot” controls give us a reasonable anti-scraping posture for public downloads and pages, even if nothing on the open web is truly copy-proof. Recent product updates explicitly target AI crawlers, with default blocking and challenge flows we can enable. (Cloudflare Docs, WIRED, Business Insider).\nFinally, the “preferences and licenses” story fits the stack. Dataset cards and metadata natively expose a license field and other tags (Hub UI and YAML), and CC licenses give clear obligations (e.g., BY, BY-SA) we can enforce in packaging and docs. That lets us partition releases by license and publish compatibility notes in a way downstream users can actually follow. (Hugging Face, Hugging Face)",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "2a_mvp.html#disadvantages",
    "href": "2a_mvp.html#disadvantages",
    "title": "8  The Serverless + Git MVP",
    "section": "8.3 Disadvantages",
    "text": "8.3 Disadvantages\nContributors have to believe the middle layer won’t silently drop or reshape submissions. If/when we introduce event-driven ingestion (queues/streams), we must design for retries and duplicates.\nUX isn’t perfectly “instant.”” There’s an inherent gap between a user pressing “share” and seeing their item on the public site, because we run validation and batching. That’s a conscious choice, but we should set expectations and likely provide some kind status updates.\nOperationally, serverless isn’t “no ops,” it’s “different ops.”” Cold starts exist, API limits are real on the platforms we hit (#todo investigate / reach out to HF), and “pay per use” can surprise us at scale without cost guardrails (see e.g. GitHub Docs).\nThere’s also platform coupling to be consider. Using specific hosted CI/CD, serverless runtimes, and hub APIs creates a degree of vendor lock-in; this is a known trade-off in serverless architectures and something we can blunt with portable formats (JSONL), documented exports, and “boring” interfaces (Git).\nCompliance costs remain non-zero. Deletions across mirrored artifacts (Hub revisions, static site snapshots, downstream forks) require a clear policy and a repeatable playbook. For licenses, we’re responsible for honoring CC obligations in our packaging and comms (e.g., keeping BY attribution fields intact; not mixing BY-SA content into incompatible bundles). CC’s legalcode and guidance make those obligations explicit; our tooling should, too. (Creative Commons)",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "2a_mvp.html#other-reading",
    "href": "2a_mvp.html#other-reading",
    "title": "8  The Serverless + Git MVP",
    "section": "8.4 Other reading:",
    "text": "8.4 Other reading:\n\nhttps://arxiv.org/abs/2109.02846\nhttps://datascience.codata.org/articles/10.5334/dsj-2021-012",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html",
    "href": "2b_data_policy.html",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "",
    "text": "9.1 Glossary of Defined Terms (for this Chapter)\n“{The Public AI Chat Frontend}” or “Frontend” means the hosted interface described in this document that allows users to issue prompts to third-party model endpoints.\n“Open WebUI Instance” or “OWUI” means the hosted Open WebUI application at {{app_link}} (or successor URLs) that provides optional accounts and chat history.\n“Opt-in Data Flywheel” or “Flywheel” means the separate contribution and distribution platform at https://optinflywheel.com (or successor URLs) through which users may opt in to contribute data for public evaluation and research use.\n“Provider(s)” or “Model Endpoint(s)” means third-party model services (e.g., national labs, commercial providers) that receive user prompts and return model outputs. The Frontend is a gateway to these services and does not control their retention, training, or use practices.\n“Model Gateway” means the service layer that forwards requests from the Frontend to Providers and records a Request Envelope (metadata such as request ID, model ID, token counts, latency, and status).\n“Request Envelope” means non-content request metadata retained for reliability, capacity, and SLO monitoring.\n“Session Telemetry” means minimal first-party analytics collected on page load/navigation (e.g., timestamp, pseudonymous session ID, coarse locale, feature flags).\n“Security Logs” means IP address and User-Agent records used for rate-limiting and abuse detection.\n“Chat Object” means prompt(s), tool calls (if any), and model output(s) associated with a session or account within the Open WebUI Instance.\n“Contribution” means any data a user intentionally submits to the Flywheel (e.g., prompt/output pairs, tags, corrections), along with per-item License and AI Preference Signal selections.\n“AI Preference Signal” means an AI-use preference value the contributor attaches to a Contribution (e.g., IETF AI Preferences draft values and/or Creative Commons preference signals), intended to be conveyed downstream.\n“License” means the Creative Commons license selected by the contributor for a Contribution (supported in the MVP: CC0-1.0, CC-BY-4.0, CC-BY-SA-4.0).\n“License Bucket(s)” means the partitioning of Contributions into separate release artifacts by License (e.g., v1.0-cc0, v1.0-cc-by, v1.0-cc-by-sa).\n“Waiting Room” means the Flywheel’s gated staging directory (e.g., _waiting_room/ in a Hugging Face repository) where Contributions are first written prior to validation and release.\n“Release” means a published version of the dataset (and associated notes/checksums) assembled from validated Contributions, partitioned by License.\n“Gated Repository” means the Hugging Face dataset repository that requires an access request and acceptance of dataset-specific terms.\n“Static Site” means the public site that hosts leaderboards and provides full dataset access, with anti-scraping controls.\n“Anonymized Contributor ID” or “Pseudonym” means a non-identifying handle published with a Contribution when a contributor elects anonymity or a pseudonym instead of an OpenWebUI or HuggingFace username.\n“Personal Data” means information that identifies or can reasonably be linked to an individual; “Sensitive Personal Data” means Personal Data that is sensitive by law or policy (e.g., health, financial, precise location, government identifiers).\n“We/Us/Our” means [ENTITY NAME], the operator of the Frontend, the Open WebUI Instance, and the Flywheel.\n“You/Your” means the individual using the Frontend and/or contributing to the Flywheel, or the entity on whose behalf the individual acts.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#what-data-is-produced-when",
    "href": "2b_data_policy.html#what-data-is-produced-when",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.2 What data is produced & when",
    "text": "9.2 What data is produced & when\n\n9.2.1 Open WebUI (no account required, but optional and recommended)\nYour OpenWebUI account and all associated data are stored and managed entirely by your OpenWebUI instance. In the case of the public AI MVP, both the flyhweel and frontend will be managed by the same organization, but in theory you could use the flywheel using an entirely separate OWUI instance (a community instance, your own, etc.)\nThe Flywheel App has no access to or control over the data stored within OpenWebUI. This includes:\n\nYour OpenWebUI Account: Your login credentials, email, and user settings.\nYour Full Chat History: All conversations you have within OpenWebUI are stored on that platform’s server.\nServer Logs & Analytics: Any logs or analytics generated by the OpenWebUI platform itself.\n\nTo understand how OpenWebUI collects, uses, and retains your data, you must consult the specific Terms of Service and Privacy Policy provided by the administrator of your OpenWebUI instance.\nThis data includes:\n\nSession telemetry (minimal) #todo: Double check telemetry and provide a sample paylod to show interested users;\n\nProduced when: Page load / navigation\nIncludes: Timestamp, pseudonymous session ID, coarse locale, feature flags\nStored in: First-party analytics\nAccess: Eng/Analytics (aggregated)\nDefault retention: Raw 30 days; aggregates 13 months\n\nIP & User-Agent (security) #todo double check this\n\nProduced when: Each request\nIncludes: IP, User-Agent for rate-limit/abuse detection\nStored in: Security log store\nAccess: SRE/Security\nDefault retention: 7 days, then delete/aggregate\n\nQuery content\n\nProduced when: On submit\nIncludes: Prompt + output\nStored in: interface database\nAccess: server admin only\nDefault retention: user can delete at any time; deleted along after 30 days of user inactivity. #todo, discuss this\n\nError logs (sanitized) #todo double check this, provide example? least important to provide an example here.\n\nProduced when: On error\nIncludes: Stack trace, request ID (no prompt/output bodies)\nStored in: Log store\nAccess: Eng (least privilege)\nDefault retention: 30 days\n\nProfile and settings #todo double check this\n\nProduced when: Sign-up\nIncludes: Email/OAuth ID, display name\nStored in: User DB\nAccess: Support/Eng\nRetention: Kept until the user deletes it. If the account is inactive for 30 days, we delete the account and associated records.\n\n\n\n\n9.2.2 Data Sent from OpenWebUI to Model Gateway (to providers)\n\nRequest envelope\n\nProduced when: Each request\nIncludes: Request ID, model ID, token counts, latency, status\nStored in: Metrics DB\nAccess: Eng/SRE\nRetention: 90 days (aggregates 13 months)\n\n\n\nProvider transparency: We do not guarantee provider behavior. We clearly display the exact payload we forward (headers + body summary) and link to the provider’s own terms and policies where available. Users should review provider terms before use.\n\n\n\n9.2.3 Data Sent From OpenWebUI to the Flywheel\nWithin the OpenWebUI interface, you may have the option to explicitly opt-in and share a specific chat with this public dataset project. When you choose to do this, OpenWebUI sends a copy of that single chat to our /api/contributions/ingest endpoint.\nOnce that data is received by our application, it is handled according to the policies described in this document. The data packet we receive includes:\n\nThe chat content and metadata you selected.\nYour OpenWebUI username (for display purposes if you choose “public”).\nA unique, non-identifying provider_user_id (e.g., user-123). We use this to create a contributor_hash to group your anonymous contributions, but we never store the raw ID itself in the final dataset.\n\nThis looks like:\n\nContribution payload\n\nProduced when: On explicit opt-in\nIncludes: Prompt, model output, optional tags\nStored in: Gated staging (effectively public) → license-bucketed release (also effectively public)\nAccess: Public\nRetention: Indefinite\n\nOpenWebUI username OR HuggingFace username OR Anonymized contributor ID\n\nProduced when: On ingest\nIncludes: user provides their huggingface username or selects a pseudonym (can leave blank)\nStored in: Dataset metadata\nAccess: Public\nRetention: Indefinite\n\nRelease artifacts\n\nProduced when: On release\nIncludes: JSONL/TSV, checksums, notes\nStored in: HF (gated) and Static Site (public)\nAccess: Public (per channel)\nRetention: Indefinite\n\n\nWhen you sign in directly to this Contribution App’s web interface, you are authenticated through your Hugging Face account using the standard OAuth protocol.\n\nWhat we use: We request the openid, profile, and email scopes from Hugging Face. The application uses your public Hugging Face username (also called a “handle”) to identify you. This username is stored in a secure, encrypted session cookie in your browser.\nWhat we don’t store: Your email address and Hugging Face password are never stored or logged by our application. The session cookie is deleted when you sign out or it expires.\nWho can access it: Only the application’s backend can read your session cookie to verify you are logged in when you submit a contribution.\nRetention: This data is ephemeral and only lasts for the duration of your active session.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#server-api-data",
    "href": "2b_data_policy.html#server-api-data",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.3 Server & API Data",
    "text": "9.3 Server & API Data\nTo ensure security, stability, and prevent abuse of the Contribution App, we handle technical data related to your requests.\n\n9.3.1 Rate Limiting\nTo prevent spam and ensure the service is available for everyone, we limit the number of requests a single user can make.\n\nWhat we use: The middleware.ts file shows that we use your IP address to enforce a rate limit (e.g., 100 requests per hour). For ingestions from OpenWebUI, we rate-limit based on their API key.\nWho can access it: Your IP address is sent to Upstash Redis, our rate-limiting service provider.\nRetention: Upstash retains a record of your IP address for the duration of the rate-limiting window, which is 1 hour.\n\n\n\n9.3.2 Server Logs\nLike most web applications, our hosting platform (Vercel) automatically generates server logs for every request.\n\nWhat they contain: These logs may include your IP address, user-agent string (browser information), the requested URL, response status code, and other standard request headers. If an error occurs, the log might contain details about the error to help us debug.\nWho can access it: Project Maintainers can access these logs through the Vercel dashboard for debugging and monitoring purposes.\nRetention: Log retention is determined by Vercel’s platform policies, which is typically between 1 and 30 days.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#event-timeline-how-data-flows",
    "href": "2b_data_policy.html#event-timeline-how-data-flows",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.4 Event timeline (how data flows)",
    "text": "9.4 Event timeline (how data flows)\n\nUser prompts a model → payload sent to model provider; query and response are saved if user made an optional OpenWebUI account.\nIf user wishes to select a chat to share via opt-in, they can do so. They never have to. Licensing and preference signals are set at opt-in time.\nOpt-in chat goes to “waiting room”\nProcessing script collects items from waiting room, applies some checks (PII, content moderation), moves them into release buckets.\nA separate processing script takes data from the gated HF repo and builds the static site with leaderboard and data dump\nPost-release: approved removals are honored in future releases and our mirrors; prior downloads may persist.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#more-on-the-flywheel",
    "href": "2b_data_policy.html#more-on-the-flywheel",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.5 More on the flywheel",
    "text": "9.5 More on the flywheel\nThis is the core data lifecycle for every chat you contribute. Your submission goes through several automated stages before it becomes a permanent part of the public dataset.\n\n9.5.1 Phase 1: Submission\nWhether you use the web form or contribute via OpenWebUI, you provide the data for a single conversation.\n\n\n9.5.2 Phase 2: The Waiting Room (Temporary)\nImmediately after submission, your contribution is packaged into a single JSON file and uploaded to a private _waiting_room directory in our Hugging Face dataset repository.\n\nWhat’s in the file: This file contains the raw content and metadata from your submission. To uniquely and privately identify contributors, we generate a contributor_hash:\n\nFor web submissions, this is a SHA256 hash of your Hugging Face username combined with a secret salt: sha256(salt + hf_username).\nFor OWUI submissions, this is a hash of the provider and your provider user ID: sha256(salt + \"openwebui:user-123\").\n\nWho can access it: Only Project Maintainers with access to the repository can see these files.\nRetention: These files are temporary. They exist only until the processing script runs, typically within a few minutes or hours, after which they are deleted.\n\n\n\n9.5.3 Phase 3: Processing & PII Redaction\nA script (process_pending.ts) periodically runs to process every file in the waiting room. Its primary job is to validate the data and automatically redact Personally Identifiable Information (PII) from the chat content. Our redaction script looks for and replaces the following patterns:\n\nEmail addresses\nIP addresses (IPv4 & IPv6)\nSocial Security Numbers (SSN)\nIBAN bank account numbers\nEthereum and Bitcoin wallet addresses\nPhone numbers\nCredit card numbers (verified with Luhn check)\nSimple name patterns (e.g., “my name is John Doe”)\n\nIf more than 5 potential PII hits are found, the file is moved to quarantine for safety.\n\n\n9.5.4 Phase 4: The Quarantine Zone\nIf a submission fails processing, it’s moved to a private _quarantined directory.\n\nWhy it’s quarantined: A file is quarantined if it is malformed (e.g., invalid JSON), fails our content validation rules, or has too many PII hits detected by the redaction script.\nWho can access it: Only Project Maintainers can access these files to diagnose processing errors.\nRetention: Quarantined files are kept indefinitely for manual review.\n\n\n\n9.5.5 Phase 5: The Final Public Dataset (Permanent) ✅\nAfter successful processing and PII redaction, your contribution is appended to a monthly JSON Lines file (e.g., data/2025-08.jsonl).\n\nWhat it is: This is the final, permanent, and clean version of your contribution. The content has been redacted, and the metadata is structured according to our public schema.\nWho can access it: This dataset is public. Anyone in the world can view, download, and use it according to the license you chose for your contribution.\nRetention: Contributions to this dataset are perpetual and irrevocable, as stated in the Terms of Service you agree to upon submission.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#licensing-preference-signals-beta",
    "href": "2b_data_policy.html#licensing-preference-signals-beta",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.6 Licensing & Preference Signals (Beta)",
    "text": "9.6 Licensing & Preference Signals (Beta)\nFor each contribution, the user selects:\n\nA Creative Commons license: CC0-1.0, CC-BY-4.0, or CC-BY-SA-4.0.\nAn AI preference signal: an IETF AI preference (draft) value and/or CC preference signal (“IETF AI Pref combo”).\n\nHow we use these:\n\nWe record license and ai_pref per record.\nRecords are partitioned by license into separate release buckets.\nDownstream users must comply with the license; we publish compatibility notes (e.g., ShareAlike).\nUsers may set account defaults; each submission can override.\n\nBinding effect: Once a record is included in a release, that license applies to that copy.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#example-retention-schedule",
    "href": "2b_data_policy.html#example-retention-schedule",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.7 Example Retention schedule",
    "text": "9.7 Example Retention schedule\n\nWeb edge\n\nData: IP + UA\nPurpose: Abuse prevention\nRetention: 7 days raw, then delete\nDeletion path: Automatic purge\n\nWeb UI\n\nData: Minimal telemetry\nPurpose: Reliability metrics\nRetention: 7 days raw, then aggregate (counts) and delete raw logs\nDeletion path: Automatic purge\n\nGateway\n\nData: Request envelopes\nPurpose: Capacity/SLOs\nRetention: 7 days raw, then aggregate (counts) and delete raw logs\nDeletion path: Automatic purge\n\nOpen WebUI Accounts\n\nData: Profile and preferences\nPurpose: Auth/consent\nRetention: Kept until user deletes; 30-day inactivity → delete\nDeletion path: Self-service delete / auto-purge\n\nFlywheel staging bucket (the “waiting room dir”, on HF)\n\nData: Pending contributions\nPurpose: Moderation/de-duplication\nRetention: Indefinite (moved to organized buckets in same repo)\nDeletion path: Manual removal on request\n\nHF (gated)\n\nData: Released records (by license)\nPurpose: Research access\nRetention: Indefinite\nDeletion path: Exclude from future versions; coordinate HF deletion where possible\n\nStatic Site (public)\n\nData: Leaderboards and full dataset access\nPurpose: Benchmarking/transparency\nRetention: Indefinite\nDeletion path: Update/remove in future releases; past d",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "2b_data_policy.html#distribution-access-control",
    "href": "2b_data_policy.html#distribution-access-control",
    "title": "9  Opt-in Flywheel Data Policy",
    "section": "9.8 Distribution & access control",
    "text": "9.8 Distribution & access control\n\n9.8.1 Hugging Face (gated)\n\nAccess requires a request with acceptance of dataset-specific Terms of Use (no re-identification; honor license and AI preferences #todo write this).\nDeletion requests: Where possible, we use Hugging Face–native workflows (e.g., issue/repo requests, maintainers’ takedowns) to process deletions and exclude items from future versions.\n\n\n\n9.8.2 Flywheel Static Site (public)\n\nHosts leaderboards and provides full dataset access for building benchmarks.\nCloudflare anti-scraping posture: Bot Management/WAF rules, rate limits, Turnstile/JS challenges on download routes, tokenized short-lived URLs, robots/meta controls, pagination/throttling, and anomaly monitoring.\nReality check: Anti-scraping reduces but cannot prevent copying of public data. We limit risk via license partitioning, logged access flows, and clear terms.",
    "crumbs": [
      "Case Study: Low friction peer production",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Opt-in Flywheel Data Policy</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ardila, Rosana, Megan Branson, Kelly Davis, Michael Henretty, Michael\nKohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers,\nand Gregor Weber. 2019. “Common Voice: A Massively-Multilingual\nSpeech Corpus.” arXiv Preprint arXiv:1912.06670.\n\n\nBarocas, Solon, and Andrew D. Selbst. 2016. “Big Data’s Disparate\nImpact.” California Law Review 104 (3): 671–732.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. “On the Dangers of Stochastic Parrots: Can\nLanguage Models Be Too Big?” In Proceedings of the ACM\nConference on Fairness, Accountability, and Transparency (FAccT),\n610–23.\n\n\nBlodgett, Su Lin, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020.\n“Language (Technology) Is Power: A Critical Survey of\n‘Bias’ in NLP.” In Proceedings of ACL,\n5454–76.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Proceedings of the Conference on\nFairness, Accountability and Transparency (FAT*), 77–91.\n\n\nCarlini, Nicholas, Matthew Jagielski, Christopher A. Choquette-Choo,\nDaniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas,\nand Florian Tramèr. 2024. “Poisoning Web-Scale Training Datasets\nIs Practical.” https://arxiv.org/abs/2302.10149.\n\n\nCarlini, Nicholas, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn\nSong. 2019. “The Secret Sharer: Measuring Unintended Memorization\nin Neural Networks.” In Proceedings of USENIX Security\nSymposium.\n\n\nCarlini, Nicholas, Florian Tramer, Eric Wallace, Matthew Jagielski,\nAriel Herbert-Voss, Katherine Lee, Adam Roberts, et al. 2021.\n“Extracting Training Data from Large Language Models.” In\nProceedings of USENIX Security Symposium.\n\n\nCommon Crawl. 2022. “Common Crawl — Web-Scale Data for\nResearch.” https://commoncrawl.org/.\n\n\nCrawford, Kate, and Trevor Paglen. 2019. “Excavating AI: The\nPolitics of Images in Machine Learning Training Sets.” https://www.excavating.ai/.\n\n\nCreative Commons. 2023. “Understanding CC Licenses and Generative\nAI.” https://creativecommons.org/2023/08/18/understanding-cc-licenses-and-generative-ai/.\n\n\nEuropean Union. 2016. “General Data Protection Regulation (EU)\n2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\n———. 2024. “Artificial Intelligence Act.” https://eur-lex.europa.eu/.\n\n\nFederal Trade Commission. 2013. “Children’s Online Privacy\nProtection Rule (COPPA) — 16 CFR Part 312.” https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018.\n“Datasheets for Datasets.” In arXiv:1803.09010.\n\n\nGrother, Patrick, Mei Ngan, and Kayee Hanaoka. 2019. “Face\nRecognition Vendor Test (FRVT) Part 3: Demographic Effects.”\nNISTIR 8280. NIST. https://doi.org/10.6028/NIST.IR.8280.\n\n\nHolland, Sarah, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia\nChmielinski. 2018. “The Dataset Nutrition Label: A Framework to\nDrive Higher Data Quality Standards.” https://arxiv.org/abs/1805.03677.\n\n\nHubinger, Evan, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte\nMacDiarmid, Tamera Lanham, et al. 2024. “Sleeper Agents: Training\nDeceptive LLMs That Persist Through Safety Training.” https://arxiv.org/abs/2401.05566.\n\n\nIllinois General Assembly. 2008. “Biometric Information Privacy\nAct (BIPA), 740 ILCS 14.” https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004.\n\n\nISO/IEC 23894:2023 Information Technology—Artificial\nIntelligence—Risk Management. 2023. ISO/IEC.\n\n\nJackson, Brandon, B Cavello, Flynn Devine, Nick Garcia, Samuel J. Klein,\nAlex Krasodomski, Joshua Tan, and Eleanor Tursman. 2024. “Public\nAI: Infrastructure for the Common\nGood.” Public AI Network. https://doi.org/10.5281/zenodo.13914560.\n\n\nJo, Emily, and Timnit Gebru. 2020. “Lessons from Archives:\nStrategies for Collecting Sociocultural Data in Machine\nLearning.” In Proceedings of FAccT, 306–16.\n\n\nLiu, Jiacheng, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen,\nArnavi Chheda-Kothary, Huy Tran, et al. 2025. “OLMoTrace: Tracing\nLanguage Model Outputs Back to Trillions of Training Tokens.”\narXiv Preprint arXiv:2504.07096.\n\n\nMcCallister, Erika, Tim Grance, and Karen Scarfone. 2010. “Guide\nto Protecting the Confidentiality of Personally Identifiable Information\n(PII).” SP 800-122. NIST.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.” In\nProceedings of the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT), 220–29.\n\n\nNarayanan, Arvind, and Vitaly Shmatikov. 2008. “Robust\nde-Anonymization of Large Sparse Datasets.” In Proceedings of\nthe IEEE Symposium on Security and Privacy, 111–25.\n\n\nNissenbaum, Helen. 2004. “Privacy as Contextual Integrity.”\nWashington Law Review 79 (1): 119–57.\n\n\nNIST. 2023. “Artificial Intelligence Risk Management Framework (AI\nRMF 1.0).” NIST AI 100-1. National Institute of Standards;\nTechnology; https://www.nist.gov/ai.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53.\n\n\nOWASP. 2023. “OWASP Top 10 for Large Language Model\nApplications.” https://owasp.org/www-project-top-10-for-large-language-model-applications/.\n\n\nRakova, Bogdana, Renee Shelby, and Megan Ma. 2023.\n“Terms-We-Serve-with: Five Dimensions for Anticipating and\nRepairing Algorithmic Harm.” Big Data & Society 10\n(2): 20539517231211553.\n\n\n“Rosenbach v. Six Flags Entertainment Corp.” 2019. 2019 IL\n123186, Supreme Court of Illinois.\n\n\nSchuhmann, Christoph, Romain Beaumont, Richard Vencu, Cade Gordon, Ross\nWightman, Mehdi Cherti, Theo Coombes, et al. 2022.\n“LAION-5B: An Open Large-Scale Dataset for Training\nNext CLIP Models.” In Proceedings of NeurIPS\nDatasets and Benchmarks.\n\n\nSelbst, Andrew D., Danah Boyd, Suresh Venkatasubramanian Friedler,\nSuresh Venkatasubramanian, and Janet Vertesi. 2019. “Fairness and\nAbstraction in Sociotechnical Systems.” In Proceedings of the\nACM Conference on Fairness, Accountability, and Transparency\n(FAccT), 59–68.\n\n\nShelby, Renee, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar\nRostamzadeh, Paul Nicholas, N’Mah Yilla-Akbari, et al. 2023.\n“Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy\nfor Harm Reduction.” In Proceedings of the 2023 AAAI/ACM\nConference on AI, Ethics, and Society, 723–41. AIES ’23. New York,\nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/3600211.3604673.\n\n\nSweeney, Latanya. 2000. “Simple Demographics Often Identify People\nUniquely.” Carnegie Mellon University, Data Privacy Working\nPaper.\n\n\nU.S. Copyright Office. 2024. “Copyright and Artificial\nIntelligence: Policy Studies and Guidance.” https://copyright.gov/ai/.\n\n\nU.S. Department of Education. 1974. “Family Educational Rights and\nPrivacy Act (FERPA).” https://www2.ed.gov/policy/gen/guid/fpco/ferpa/index.html.\n\n\nU.S. Department of Health and Human Services. 2000. “HIPAA Privacy\nRule — 45 CFR Parts 160 and 164.” https://www.hhs.gov/hipaa/for-professionals/privacy/index.html.\n\n\nWeidinger, Laura, John Mellor, et al. 2021. “Ethical and Social\nRisks of Harm from Language Models.” arXiv Preprint\narXiv:2112.04359.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix1_llm_data.html",
    "href": "appendix1_llm_data.html",
    "title": "10  Appendix 1: LLM Data Schemas",
    "section": "",
    "text": "Here, we describe many variants of LLM data. This will be relevant for when we extend the flywheel to include more types of data, and especially shift towards promoting the sharing (via opt-in flywheels, but also via new market mechanisms) of richer “content data”.\n\nOpen Web / Crawls\n\nWARC/WAT/WET\n\nWARC (container for HTTP request/response records) — spec & overview: IIPC WARC 1.1; Library of Congress format note. (IIPC Community Resources, The Library of Congress)\nWAT (JSON metadata extracted from WARC) and WET (plain text extracted from HTML) — Common Crawl guides. (Common Crawl, Common Crawl)\n\nC4 (Colossal Clean Crawled Corpus) — TFDS catalog & generator code. Fields are essentially clean text segments with basic metadata. (TensorFlow, GitHub)\nThe Pile (22-source, mixed corpus) — paper & HTML view. (arXiv, ar5iv)\n\nEncyclopedic / Books\n\nWikipedia XML dumps (page/revision XML; SQL tables for links) — Meta-Wiki dump format; Wikipedia database download. (Meta, Wikipedia)\nProject Gutenberg\n\nBooks: plain text/HTML master formats; ePub/MOBI derived. (Project Gutenberg)\nCatalog schema: daily RDF/XML (also CSV) for metadata; offline catalogs. (Project Gutenberg)\n\n\nScientific / Legal\n\narXiv (Atom/OAI-PMH metadata; bulk & API) — OAI-PMH + API docs; bulk metadata page. (info.arxiv.org, info.arxiv.org, info.arxiv.org)\nJATS XML (journal article tag suite) — NISO standards; NLM JATS site. (niso.org, jats.nlm.nih.gov)\n\nCode\n\nBigCode — The Stack / The Stack v2 (source files + license/provenance metadata; dedup variants) — HF datasets, project docs, arXiv overview. (Hugging Face, Hugging Face, BigCode, arXiv)\n\nForums / Q&A / Social\n\nStack Exchange dumps (XML: Posts, Users, Comments, Votes, etc.) — SE Meta/docs & Data Explorer. (Meta Stack Exchange, data.stackexchange.com)\nReddit\n\nAPI JSON schema — official API docs & help. (Reddit, Reddit Help)\nPushshift (historical dumps; research dataset) — site & paper. (pushshift.io, arXiv)\n\n\nInstruction / Conversations (Post-training SFT)\n\nOpenAI-style chat schema (role-tagged: system|user|assistant, plus tool calls) — API reference. (OpenAI Platform)\nAlpaca (JSON prompts/instructions/outputs) — Stanford post & repo; cleaned community set. (crfm.stanford.edu, GitHub, GitHub)\nDatabricks Dolly-15k (human-written instruction/response pairs) — repo. (GitHub)\nOpenAssistant OASST1 (message-tree conversations with roles) — HF dataset card. (Hugging Face)\n\nPreference / Feedback (RLHF & DPO)\n\nHH-RLHF (Anthropic helpful/harmless, JSONL pairs: chosen vs rejected) — dataset repo readme. (GitHub)\nDPO format (prompt + preferred vs dispreferred response) — DPO paper. (arXiv)\n\nMultimodal (for VLMs/ASR)\n\nLAION-5B / Re-LAION-5B (image–text pairs with CLIP scores; links) — LAION posts. (laion.ai, laion.ai)\nWhisper (weakly-supervised ASR; audio → text pairs) — paper & blog. (arXiv, OpenAI)\nHowTo100M (YouTube instructional video clips + narrations) — project page & paper. (di.ens.fr, arXiv)\n\nMath-reasoning (often for post-training/eval)\n\nGSM8K (grade-school word problems; JSON) — repo & HF dataset card. (GitHub, Hugging Face)\nMATH (competition problems with step-by-step solutions) — paper & HF. (arXiv, Hugging Face)\n\nCommon storage containers\n\nJSON Lines / NDJSON — jsonlines.org; ndjson spec. (jsonlines.org, GitHub)\nTFRecord — TensorFlow tutorial. (TensorFlow)\nApache Parquet — project site. (Apache Parquet)\n\n\n#todo check all refs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix 1: LLM Data Schemas</span>"
    ]
  },
  {
    "objectID": "appendix2_prefsig.html",
    "href": "appendix2_prefsig.html",
    "title": "11  Appendix 2 — Preference Signals for AI Data Use (CC signals + IETF AI Preferences)",
    "section": "",
    "text": "#todo: improve the references here to specific lines of IETF draft and the CC Preference Signals FAQ\n\nWhat CC signals are A Creative Commons framework for reciprocal AI reuse: content stewards can allow specific machine uses if certain conditions are met (e.g., credit, contributions, openness). Overview & implementation notes. (homepage, implementation)\nFour proposed CC signals (v0.1)\n\nCredit (cc-cr) — cite the dataset/collection; RAG-style outputs should link back when feasible.\nCredit + Direct Contribution (cc-cr-dc) — proportional financial/in-kind support.\nCredit + Ecosystem Contribution (cc-cr-ec) — contribute to broader commons.\nCredit + Open (cc-cr-op) — release model/code/data to keep the chain open. Source (draft repo & posts). (GitHub, Creative Commons)\n\nIETF AI Preferences (aipref) — the transport & vocabulary\n\nVocabulary: a machine-readable set of categories (e.g., ai-use, train-genai) and preferences (y = grant, n = deny) with exceptions. Drafts. (datatracker.ietf.org, IETF, IETF AI Preferences Working Group)\nAttachment: how to convey these preferences via HTTP Content-Usage header and robots.txt extensions. Drafts. (datatracker.ietf.org, IETF)\nStructured Fields: uses RFC-standardized HTTP structured field values. (datatracker.ietf.org, datatracker.ietf.org, rfc-editor.org)\nRobots Exclusion Protocol baseline. (datatracker.ietf.org, rfc-editor.org)\n\nPutting them together (content-usage expression)\n\nShape:\n&lt;category&gt;=&lt;y|n&gt;;exceptions=&lt;cc-signal&gt;\nExample in robots.txt (allow everything, but AI use denied unless Credit):\nUser-Agent: *\nContent-Usage: ai-use=n;exceptions=cc-cr\nAllow: /\nExample HTTP header (deny gen-AI training unless Credit + Ecosystem):\nContent-Usage: train-genai=n;exceptions=cc-cr-ec\n(Syntax and examples from CC & IETF drafts.) (Creative Commons, IETF)\n\nOperational notes (for this repo’s flywheel)\n\nPer-record fields to store: license (CC0/CC-BY/CC-BY-SA) and ai_pref (IETF aipref value + optional CC signal), plus optional attribution handle. (Aligns with CC write-ups & IETF drafts.) (Creative Commons, datatracker.ietf.org)\nPlacement:\n\nLocation-based signals via robots.txt for site/paths. (datatracker.ietf.org)\nUnit-based signals via HTTP Content-Usage on dataset files and API responses. (datatracker.ietf.org)\n\nInteroperability expectations: signals are normative preferences; adherence relies on ecosystem norms (similar to robots.txt & CC license culture). (Creative Commons)\n\nContext & momentum\n\nCC’s 2025 launch posts; IETF WG activity updates (e.g., IPTC note). (Creative Commons, Creative Commons, IPTC)\n\n\n\n#todo check all refs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Appendix 2 — Preference Signals for AI Data Use (CC signals + IETF AI Preferences)</span>"
    ]
  },
  {
    "objectID": "appendix3_example_legalterms.html",
    "href": "appendix3_example_legalterms.html",
    "title": "12  Appendix 3: Example Legal Terms",
    "section": "",
    "text": "12.1 Opt-in Data Flywheel — Legal Terms (Draft)\nEffective: [DATE]\nThrough the Opt-in Data Flywheel, you may contribute chats, corrections, and related materials to build openly accessible evaluation sets and datasets.\nYou may participate only if you agree to these Opt-in Data Flywheel Legal Terms (the “Terms”). 1. Eligibility\nThe Flywheel is open to individuals who are the age of majority in their jurisdiction, or to younger participants with verified parental/guardian consent and supervision. You must also comply with Our Community Guidelines/Acceptable Use Policy ([LINK]). 2. Your Contributions; Licensing; AI Preferences\n2.1 Opt-in Only. Submitting to the Flywheel is purely voluntary and separate from using the Open WebUI Instance.\n2.2 License Grant (per item). For each Contribution, you select one License (CC0-1.0, CC-BY-4.0, or CC-BY-SA-4.0). You grant Us a non-exclusive, worldwide right to publish, reproduce, modify (solely for formatting, moderation, and aggregation), distribute, and sublicense the Contribution under the selected License. Once included in a Release, that License applies to that copy of the Contribution.\n2.3 AI Preference Signals. If you attach an AI Preference Signal, We will transmit and display it with the Contribution and document how Our systems interpret such signals. We cannot guarantee that downstream users or Providers will honor such signals.\n2.4 Assurances. You represent and warrant that (a) you have the necessary rights to your Contributions; (b) your Contributions do not infringe third-party rights; (c) you will not include Sensitive Personal Data; and (d) you will comply with Our Acceptable Use Policy. 3. Accounts; Attribution; Pseudonymity\n3.1 Auth. Contributions require authentication (e.g., Hugging Face OAuth). 3.2 Attribution. You may choose to publish under your Hugging Face username, under a pseudonym, or as “anonymous.” 3.3 Leaderboards. We may publish contribution metrics (counts, languages, tags) with your chosen public handle. We will not publish your email address. 4. Processing; Waiting Room; Release\n4.1 Waiting Room. Submissions write to a staging directory. 4.2 Validation. We may run automated and human review for formatting, de-duplication, PII/safety checks, and License/AI-preference validation. 4.3 Release. Validated items are appended to License Buckets (e.g., vYYYY-MM) and published to a Gated Repository and mirrored to the Static Site. 5. Distribution; Access Control\n5.1 Gated Repository. Access requires acceptance of dataset-specific terms (e.g., no re-identification; respect License and AI preferences). 5.2 Static Site. Public access includes anti-scraping measures (WAF/bot management, rate limits, tokenized URLs). Copying cannot be fully prevented; rely on License controls for downstream obligations. 6. Deletions & Takedowns\n6.1 Future-Only Removal. Upon verified request, We will exclude the identified Contribution(s) from future Releases and update mirrors where feasible. Past Releases and third-party copies may persist. 6.2 Hugging Face Workflows. Where possible, We will route or honor takedowns via the Hugging Face repository’s native workflows. 7. Provider Transparency (No Guarantees)\nWe forward prompts to third-party Providers. We display a payload transparency panel and link to Provider terms when available. We do not control Provider retention, training, or other uses of data once sent to them. 8. Privacy; Retention\nRetention and access for telemetry, envelopes, accounts, staging, Releases, and the Static Site are governed by the Data Retention & Contribution Policy (Section 3). That Policy is incorporated by reference. 9. Communications\nBy creating an account or requesting repository access, you may receive administrative emails (e.g., access decisions, policy updates). 10. Disclaimers; Limitation of Liability; Indemnity\nTHE FLYWHEEL AND RELEASES ARE PROVIDED “AS IS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES (INCLUDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT). WE WILL NOT BE LIABLE FOR INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY, OR PUNITIVE DAMAGES. Our aggregate liability under these Terms will not exceed USD $500 (or the maximum permitted by law if lower). You agree to indemnify Us for third-party claims arising from your Contributions or breach of these Terms. 11. Updates\nWe may update these Terms by posting a new effective date. Continued use after the effective date constitutes acceptance. 12. Termination\nWe may suspend or terminate access at any time. Contributions included in prior Releases remain available under their Licenses. 13. Governing Law; Venue\nThese Terms are governed by the laws of [LAW & VENUE], without regard to conflict-of-laws rules. Exclusive venue lies in the courts of [VENUE].",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Appendix 3: Example Legal Terms</span>"
    ]
  },
  {
    "objectID": "appendix3_example_legalterms.html#frontend-instance",
    "href": "appendix3_example_legalterms.html#frontend-instance",
    "title": "12  Appendix 3: Example Legal Terms",
    "section": "12.2 Frontend Instance",
    "text": "12.2 Frontend Instance\nOpen WebUI Instance — Terms of Use (Draft)\nEffective: [DATE]\nThese Terms govern your use of Our hosted Open WebUI Instance at {{app_link}} (or successor URLs). 1. Eligibility; Community Rules\nThe service is available to individuals who are the age of majority in their jurisdiction, or younger participants with verified parental/guardian consent and supervision. You must follow Our Community Guidelines/Acceptable Use Policy ([LINK]). 2. Accounts; Content\n2.1 Accounts Optional. You may use OWUI without an account; certain features (history, settings, opt-in share flows) require an account. 2.2 Your Content. Prompts and outputs in your account are stored as Chat Objects to provide history and UX features. They are not used for training or evaluation by Us unless you explicitly opt in via the Flywheel. 2.3 Feedback Data. Thumbs, flags, and similar signals may be stored to improve product reliability and moderation and are handled per Section 3 (Retention Policy). 3. Provider Transparency\nOWUI forwards your prompts to third-party Providers. We display a payload transparency panel and, where available, links to Provider terms. We do not control Provider retention, training, or other uses of your data. 4. Privacy; Retention; Security\nRetention, deletion, and access controls for telemetry, Security Logs, Request Envelopes, account data, and error logs are governed by the Data Retention & Contribution Policy (Section 3), incorporated here by reference. 5. Sharing to the Flywheel\nSharing to the Flywheel is separate and requires explicit opt-in with per-item License and AI Preference Signal selections. See the Flywheel Terms. 6. Acceptable Use\nYou agree not to: (a) upload Sensitive Personal Data; (b) violate laws or third-party rights; (c) attempt to reverse engineer or abuse rate limits; (d) circumvent access controls; or (e) interfere with service integrity. 7. Communications\nIf you create an account, We may send administrative emails (e.g., login links, security alerts, policy updates). 8. Disclaimers; Limitation of Liability\nOWUI IS PROVIDED “AS IS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES (INCLUDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT). WE WILL NOT BE LIABLE FOR INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY, OR PUNITIVE DAMAGES. Our aggregate liability will not exceed USD $500 (or the maximum permitted by law if lower). 9. Updates; Termination\nWe may update these Terms by posting a new effective date. Continued use after the effective date constitutes acceptance. We may suspend or terminate accounts for any reason, including AUP violations or security risk. 10. Governing Law; Venue\nThese Terms are governed by the laws of [LAW & VENUE], without regard to conflict-of-laws rules. Exclusive venue lies in the courts of [VENUE].",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Appendix 3: Example Legal Terms</span>"
    ]
  }
]