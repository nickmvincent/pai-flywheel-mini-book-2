[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Flywheels and Public AI",
    "section": "",
    "text": "Preface\nThis is a “mini-book” that discusses “public AI flywheels”: software meant to enable people to opt-in to contribute data towards “public AI” causes. The goal of this book is to support efforts build a transparent, people-centric data collection ecosystem that supports the evaluation and training of public-benefit AI models. More frankly, this is way to organize some design notes, practical documentation that’s out of scope for a single example projec’s repo, and longer abstract writing on the topic.\nThis document is organized as such:\n\nIn Section 1-4, we first we describe how this document is organized and introduce the Public AI Flywheel concept.\nIn Sections 5-7, we discuss one particular implementation of a Minimum Viable Product (MVP) opt-in flywheel meant to accompany a “public AI interface” (hosted interface software that hits various endpoints for “public AI models”) that uses a “serverless” app + Git backend approach\n\nThis MVP focuses on collecting two high-signal data types: exports of “good chats” and “fail chats.” This data provides immediate value for model evaluation and, at scale, can be used for fine-tuning. Importantly, collecting a list of good and bad chats is also immediately fun, so contributors can get some value before we reach a threshold of data volume needed to construct a full benchmark or dataset. We expect key ideas discussed in this doc, and concretized in this project, to generalize to other data types.\nWe also provide details on the data retention policy for the Public AI Flywheel and the data policy for the full public AI interface pipeline: from model endpoints to OpenWebUI interface to flywheel platform.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01a_intro.html",
    "href": "01a_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is a data flywheel?\nWhat is a data flywheel? Nvidia gives us this definition: “A data flywheel is a feedback loop where data collected from interactions or processes is used to continuously refine AI models.” Others have also written on data flywheels (see e.g. a number of helpful blogs from Roche and Sasson, Liu and Del Balso.\nIn general, a “data flywheel” is a system or set of systems that capture and/or incentive data. A “flywheel” generally differs from a more general data collection or data creation system because the flywheel is embedded into some kind of application (as opposed to e.g. “standalone” data labeling tasks).\nGenerally, most data collection systems lean more towards using “sensors” (passive, instruments, no active “submit data” step) or forms (active, requiring somebody to click “submit”). Flywheels tend to be lean passive, but this is not necessarily a requirement.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#what-is-a-data-flywheel",
    "href": "01a_intro.html#what-is-a-data-flywheel",
    "title": "1  Introduction",
    "section": "",
    "text": "For some further thoughts on sensors and forms, see this post",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#what-is-a-public-ai-data-flywheel",
    "href": "01a_intro.html#what-is-a-public-ai-data-flywheel",
    "title": "1  Introduction",
    "section": "1.2 What is a public AI data flywheel?",
    "text": "1.2 What is a public AI data flywheel?\nFirst, what is “public AI”? The public AI network gives us this: AI with\n\n“Public Access – Certain capabilities are so important for participation in public life that access to them should be universal. Public AI provides affordable access to these tools so that everyone can realize their potential.” “Public Accountability – Public AI earns trust by ensuring ultimate control of development rests with the public, giving everyone a chance to participate in shaping the future.” “Permanent Public Goods – Public AI is funded and operated in a way to maintain the public goods it produces permanently, enabling innovators to safely build on a firm foundation.”\n\nFor more on the public AI concept, see also Mozilla’s work in this space and several workshop papers and preprints (from RegML 2023 at NeurIPS, CodeML 2025 at ICML, a recent workshop on Canadian Internet Policy).\nOur focus in this mini-book is building “public AI” flywheels. To summarize heavily – in trying to achieve all the principles laid out in the above work that tries to define “public AI”, those building public AI flywheel will face some unique challenges in the implementation of data flywheels; they may not be able to do what private orgs can do.\nIn building public AI data flywheels, we are trying to create a feedback loop to improve AI. However, we likely want to start from a position of accessibility (including providing an accessible explanation of exactly what happens to any data a user creates) and accountability (so people have real agency over the shape of the data pipeline).\nOf course, it’s worth noting that some particular public could deliberate and make a collective decision that they prefer a more “traditional approach” to data. In this mini-book, we are taking the stance that it’s best to start from a position of leaning heavily towards an opt-in approach. We start by minimizing usage and retention of data; data that is used in the flywheel to train AI should be provided via an opt-in by highly informed users.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01a_intro.html#core-principles",
    "href": "01a_intro.html#core-principles",
    "title": "1  Introduction",
    "section": "1.3 Core Principles",
    "text": "1.3 Core Principles\nWe can translate the core principles of public AI to the data flywheel domain and arrive at roughly four requirements:\n\nTransparency for Informed Consent: Users must be fully informed about the model, its developer, and the ramifications of their contribution. A detailed FAQ and a clear consent module are required before any data is shared. To some extent, maximally informed consent will require the active expenditure of resoures to improve the public’s AI literary (i.e. we need to build AI literacy focused systems and perhaps even pay people for their attention). We need systems that really do inform people. Luckily, that’s something it seems like AI can help with!\nData Rights: A public AI data flywheel must empower users with control over their data, mirroring GDPR principles and similar regulations (this is also practically important for compliance). This includes the right to access ($Art. 15$), rectify ($Art. 16$), erase ($Art. 17$), and port their data ($Art. 20$). Key exemplar: Mozilla Common Voice).\n\nWe note that data rights sometimes conflict with a “fully open” ethos; we will attempt to mitigate these tensions to the best extent possible!\nWe also note that public AI faces some unique challenges with cross-jurisdiction compliance; we discuss this at a high-level later on. #todo link exact section\n\nBalancing reputation and pseudonymity: To the extent possible, we believe it is valuable to offer people to ability to contribute to data flywheel with some kind “real account” attached, so people can earn credit and reputation. But this must be balanced with the benefits of “light-auth” or even anonymous contribution (#todo cite CDSC work on anon contributions).\n\nIn our MVP (discussed in the next chapter) an OpenWebUI or HuggingFace account is required to make contributions, but users can choose to remain anonymous or use a pseudonym for the public data release.\n\nPurpose Limitation & Licensing: Users must be able to specify their preferences for how their data is used (e.g., for public display and evaluation vs. for model training). This is captured using (new) IETF AI Use Preferences and Creative Commons Preference Signals. We will discuss below how this might extend to other preference signal proposals and/or technical approaches to gating data.\n\nThis is critical for answering a likely FAQ around public AI data – if you succeed in creating actually useful training data or new benchmarks, won’t private labs just immediately use that data as well?",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01b_what_data_and_why.html",
    "href": "01b_what_data_and_why.html",
    "title": "2  What data and why",
    "section": "",
    "text": "2.1 An overly detailed accounting of all the ways we might generate LLM pre-training data\nLLM pre-training data consists of (at a high level):\nAt the lowest level, a person does something that leaves a digital trace: typing, speaking into a mic, using some kind of alternate controller, etc. They might also operate a camera or other sensing instrument that captures signals from the world.\nAfter typing (or other input), they might use a terminal or GUI to send their inputs into some structure – by committing code, editing a wiki, responding on a forum).\nOften, this person has a goal and/or a task they want to complete—ask a question, teach or correct something, build software, file a bug, summarize a meeting, translate a passage, or simply react (like/flag/skip).\nExamples include:",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What data and why</span>"
    ]
  },
  {
    "objectID": "01b_what_data_and_why.html#an-overly-detailed-accounting-of-all-the-ways-we-might-generate-llm-pre-training-data",
    "href": "01b_what_data_and_why.html#an-overly-detailed-accounting-of-all-the-ways-we-might-generate-llm-pre-training-data",
    "title": "2  What data and why",
    "section": "",
    "text": "Human-authored natural language: the open web (cleaned), books, encyclopedias, news, forums/Q&A, transcripts (talks, meetings, podcasts), documentation, and manuals.\n\nAnd now, some non-human-authored natural language.\n\nCode: source files, perhaps with licenses and provenance, issue threads, commit messages.\nSemi-structured text: tables, markup, configs (HTML/Markdown/LaTeX/YAML/JSON) that carry schema and relationships.\nMultimodal pairs (for VLM/ASR pretraining): image+text, audio+text, video+text, and associated captions/alignment.\n\nHere, the pairing is a critical characteristic that makes this data unique. This implies somebody has looked at the each item in the pair and confirmed a connection (though paired data can be produced in an automated fashion).\n\nMetadata about data: language, domain/topic tags, timestamps, links, authorship/attribution, license, and (in our case) AI preference signals.\nQuality signals: dedup scores, perplexity filters, toxicity/PII flags, heuristic or model-based ratings—used to weight or exclude.\n\n\n\n\n\n\nAsking a model a question and marking the response “good” or “fail”, optionally with a short note about why.\nCorrections/edits: rewriting a wrong answer; adding a missing citation; supplying a step-by-step solution.\nPairwise preferences: “A is better than B because …” (useful for preference learning/DPO).\nStar ratings / rubrics: numeric or categorical grades on axes like factuality, helpfulness, tone, safety.\nTags and metadata: topic (“tax law”), language (“id-ID”), difficulty (“HS”), license (CC-BY-SA), and AI preference signals.\nSynthetic tasks: user-written prompts + ideal references (gold answers, test cases, counterexamples).\nMultimodal: an image with a caption; an audio clip with a transcript; a diagram with labeled parts.\nProgrammatic contributions: code snippets with docstrings/tests; minimal reproductions of a bug.\n“Negative” structure: anti-patterns, jailbreak attempts, hallucination catalogs.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What data and why</span>"
    ]
  },
  {
    "objectID": "01b_what_data_and_why.html#some-other-examples-of-data",
    "href": "01b_what_data_and_why.html#some-other-examples-of-data",
    "title": "2  What data and why",
    "section": "2.2 Some other examples of data",
    "text": "2.2 Some other examples of data\n\nExplicit numerical ratings of items\nImplicit feedback about items (clicks, dwell time, scroll/hover, skips/abandonment—when captured with consent and documented)\nEdits/diffs (how a draft changes—great signal for learning to revise)\nFlags and rationales (spam, unsafe, off-topic, with short free-text)\nSearch queries + selected result (query→choice pairs; useful for intent and relevance)\nStructured forms (error taxonomy, severity, reproduction steps)\nTrace data from tools (with consent): which functions were called, which docs were opened, etc.—often better as evaluation context than as training text.\n\n[^1] A bayesian might say: data is evidence that updates a prior into a posterior via Bayes’ rule; the “goodness” of a dataset is how much information (likelihood ratio / bits of surprise) it carries about the hypotheses we actually care about. A frequentist might say: data are samples from some process; more (and more representative) samples tighten confidence intervals and reduce estimator variance (roughly with \\(1/\\sqrt{n}\\)), so sampling design and coverage matter as much as sheer volume.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What data and why</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html",
    "href": "01c_pipeworks.html",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "",
    "text": "3.1 Why a “pipeworks” view?\nMost technical AI work zooms in on a clean optimization problem. But questions about who benefits, who participates, and how AI affects society live upstream and downstream of that problem. The Data Pipeworks zooms out. It describes the end-to-end flow by which human activity becomes records, then datasets, then models embedded in systems that act on the world—and thereby change the future data we can collect. That circularity is the opening for governance.\nThis view pairs naturally with cybernetics/control: identify system state, actuators, sensors, and feedback loops; then decide which loops to strengthen, dampen, or rou",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#five-stages-of-data",
    "href": "01c_pipeworks.html#five-stages-of-data",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "3.2 Five stages of data",
    "text": "3.2 Five stages of data\n\nKnowledge & Values (Reality Signal). Humans (and the physical world) generate the latent “signal” AI tries to model (facts, preferences, norms). We don’t presume computability; we note its existence to emphasize sampling implications.\nRecords (Sampling Step). Interfaces and sensors transform activity into structured records (forms, clicks, edits, uploads; camera/mic/IoT streams). Design choices here shape what becomes legible to AI. Key idea: everything either leans “sensor” or “form”.\nDatasets (Filtering & Aggregation). Organizations filter, label, merge, and license records under social, economic, and legal constraints. This determines coverage, bias, and what’s even available to learn from.\nModels (Compression). Learning compresses datasets into input–output mappings. Model choices are path-dependent on Stages 2–3; data defines the feasible hypothesis space.\nDeployed Systems (Actuation). Models are embedded in products, workflows, or infrastructure, producing value and externalities. Deployment feeds back by altering incentives and future record creation.\n\nDesign note: because influence composes across stages, small, well-placed interventions upstream can dominate large downstream tweaks.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#why-this-matters-for-governance-and-alignment",
    "href": "01c_pipeworks.html#why-this-matters-for-governance-and-alignment",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "3.3 Why this matters for governance and alignment",
    "text": "3.3 Why this matters for governance and alignment\n\nHuman factors are primary. The distributions the AI field is optimizing over are created, not discovered. Interfaces, defaults, prompts, consent flows, and incentives shape the topology of AI work.\nSocial dilemmas are inevitable. Contributing high-quality records to a shared system is a collective action problem (free-riding, failure to reach critical mass). Today’s “dictator solution” (opaque scraping) collapses when people gain data agency.\nData leverage (next chapter) is the steering wheel. Individuals and groups can alter records, licenses, and access. This allows people to steer model behavior by modulating data flow rather than model internals.\nPluralism becomes measurable. Tracing contributions lets us quantify relative weight of individuals and communities, enabling pluralistic governance and new not",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#where-to-place-the-levers-for-public-ai-flywheels",
    "href": "01c_pipeworks.html#where-to-place-the-levers-for-public-ai-flywheels",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "3.4 Where to place the levers (for public AI flywheels)",
    "text": "3.4 Where to place the levers (for public AI flywheels)\n\nStage 1 to 2 (Knowledge to Records): invest in interfaces and sensors with informed consent; design contribution prompts and micro-tasks; support pseudonymity and reputation choices. Aim to raise signal quality and widen participation.\nStage 2 to 3 (Records to Datasets): attach licenses and AI preference signals per record; validate, de-duplicate, and redact PII; publish partitioned releases. Make rights legible and keep high-trust, high-reuse bundles.\nStage 3 to 4 (Datasets to Models): enable data markets and coalitions, attribution, and sampling weights; build evaluation sets tied to provenance. Align training with community intent and enable bargaining.\nStage 4 to 5 (Models to Systems): publish transparent deployment notes, opt-outs, and model cards tied to data buckets. Surface externalities and set expectations for use.\nStage 5 to 2 (Feedback loop): close the loop with flywheel UX. Leaderboards, grants, bounties, governance hooks (votes, preferences) to sustain contributions and invite further steering.\n\nOur MVP (“Serverless + Git” flywheel) primarily operates across 2-3-5: it turns opt-in records into licensed, preference-tagged datasets, publishes gated and public releases, and feeds visible outcomes back to contributors.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#implications-for-research-and-practice",
    "href": "01c_pipeworks.html#implications-for-research-and-practice",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "3.5 Implications for research and practice",
    "text": "3.5 Implications for research and practice\nBuilding flywheels are part of broader agenda to enable a data pipeworks. More in the next chapter on how data contribution through flywheels (including licensed or user-restricted contribution) interplays with data protection, data strikes, markets, etc.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "01c_pipeworks.html#a-compact-mental-model",
    "href": "01c_pipeworks.html#a-compact-mental-model",
    "title": "3  The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)",
    "section": "3.6 A compact mental model",
    "text": "3.6 A compact mental model\n\nSensors and interfaces decide what counts.\nFilters and markets decide what persists.\nCompression decides what generalizes.\nDeployment decides what changes next.\nGovernance decides who gets to steer.\n\nPublic AI flywheels turn that loop into a participatory control system: contributors see consequences, express preferences, and are (hopefully) rewarded for adding high-signal records.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Data Pipeworks: From Human Knowledge to Deployed AI (and Back Again)</span>"
    ]
  },
  {
    "objectID": "03_leverage.html",
    "href": "03_leverage.html",
    "title": "4  Flywheels and Markets",
    "section": "",
    "text": "4.1 How an opt-in flywheel enables markets\nAn opt-in flywheel can create the prerequisites for functioning data markets without turning the project into “just a marketplace.”\nNote: a key idea here is that it’s possible to enable market activity while keeping the data open-but-gated-and-restricted or by using the flywheel as a stepping stone towards a more “property-like” market.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "03_leverage.html#how-an-opt-in-flywheel-enables-markets",
    "href": "03_leverage.html#how-an-opt-in-flywheel-enables-markets",
    "title": "4  Flywheels and Markets",
    "section": "",
    "text": "Units and provenance Each contribution is a unit with provenance, license, usage preferences, minimal schema, and (optionally) reputation of a contributor or collective. That makes it legible enough to transact on.\nPermissioning and packaging By establishing practices around setting use requirements (e.g., “eval-only”, “train-allowed-if-model-is-open”), it is possible to either (1) provide nicely packaged releases (monthly, by topic/language/domain) of data (data user is paying for premium like Wikimedia Enterprise) or (2) collectively move from sending data to an open repo to a gated repo managed by a market agent.\nDiscovery and price signals Leaderboards, scarcity tags (rare language/domain), and quality scores effectively begin to generate price signals. A bounty board (“need 5k labeled failures in X”) converts demand into targeted supply. In this sense, bounty boards for OSS also enable market dynamics around outputs that are eventually made public with license restrictions.\nClearing and settlement Because contributions are opt-in and addressable, enterprise-style payouts can be routed to individuals or collectives, with rules like “70% to contributors pro-rata by weight; 30% to the commons.”\nCollectives as market actors Co-ops/unions/DAOs can represent contributors, negotiate bundle terms, run audits, and set default preferences. The flywheel provides the shared ledger and release cadence that markets need.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "03_leverage.html#how-an-opt-in-flywheel-enables-strikes-or-credible-refusals",
    "href": "03_leverage.html#how-an-opt-in-flywheel-enables-strikes-or-credible-refusals",
    "title": "4  Flywheels and Markets",
    "section": "4.2 How an opt-in flywheel enables strikes (or credible refusals)",
    "text": "4.2 How an opt-in flywheel enables strikes (or credible refusals)\n“Strike” here means a coordinated, temporary withdrawal or constraint on new high-signal contributions or releases—not sabotage, and not retroactive deletion of already-licensed copies.\nWhat makes strikes possible:\n\nVoluntariness is preserved. Because contribution is opt-in, non-participation is a legitimate default.\nRelease control. A waiting-room, processing, release pipeline provides a natural “valve” for cadence changes or strikes.\nShared visibility. Everyone sees dependence on fresh contributions (e.g., evaluation drift). Visibility creates leverage.\n\nWhat a strike can look like:\n\nQuality freeze. Contributors keep using systems but withhold labeled “good/fail” chats or corrections for a period.\nSelective embargo. A community with scarce data (language/domain) pauses releases or flips new records to “evaluation-only.”\nPreference shift. New contributions change AI-use preferences to deny training unless a stated condition is met (funding, governance, attribution).\nRate limit. Collectives cap monthly volume to force negotiations on price or terms.\n\nWhat a strike cannot do (and shouldn’t promise):\n\nUndo past licenses. Items released under irrevocable terms (e.g., CC0, CC-BY) remain available.\nPrevent copying entirely. Public releases can be mirrored; anti-scraping reduces risk but does not eliminate it.\nGuarantee compliance outside the ecosystem. Preference signals work when counterparties agree to honor them or when law/policy backs them.\n\nNet effect: strikes are most effective where freshness and scarcity matter (evaluations, rapidly changing domains, rare languages), and least effective where substitutes or synthetic data can plausibly fill gaps.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "03_leverage.html#preconditions-the-flywheel-must-provide",
    "href": "03_leverage.html#preconditions-the-flywheel-must-provide",
    "title": "4  Flywheels and Markets",
    "section": "4.3 Preconditions the flywheel must provide",
    "text": "4.3 Preconditions the flywheel must provide\n\nClear, per-item license and AI-use preference capture and propagation\nAuditable provenance and contributor linkage (including pseudonyms)\nRelease discipline (cadence, versioning, checksums) and the ability to pause cadence\nA shared metrics surface showing demand, scarcity, and impact\nCollective controls: settings a group can change together (defaults, embargo toggles)\nPayment rails or grant accounting tied to releases, not pageviews",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "03_leverage.html#limits-and-failure-modes-to-plan-for",
    "href": "03_leverage.html#limits-and-failure-modes-to-plan-for",
    "title": "4  Flywheels and Markets",
    "section": "4.4 Limits and failure modes to plan for",
    "text": "4.4 Limits and failure modes to plan for\n\nIrrevocability vs. agency. If the goal is strike leverage, discourage CC0 for scarce data; offer eval-only or time-boxed terms.\nLeakage. Plan for mirrors; design incentives that make compliance more attractive than defection.\nSubstitution. Some domains can be synthetically approximated; focus markets/strikes where real-world records are hard to fake.\nCoordination overhead. Without lightweight collective tools, only well-organized groups can act.\nLegal and policy constraints. Collective bargaining around data may raise jurisdiction-specific questions; stay within applicable law.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "03_leverage.html#a-practical-path-from-mvp-to-markets-and-optional-strikes",
    "href": "03_leverage.html#a-practical-path-from-mvp-to-markets-and-optional-strikes",
    "title": "4  Flywheels and Markets",
    "section": "4.5 A practical path from MVP to markets (and optional strikes)",
    "text": "4.5 A practical path from MVP to markets (and optional strikes)\n\nv0: Opt-in contributions with per-item license and AI-preference signals; public/gated releases.\nv1: Contributor dashboards, scarcity/impact badges, and a bounty board.\nv2: Collective accounts that can set defaults, pool payouts, and flip embargo/quality-freeze toggles.\nv3: Formal procurement: RFPs for datasets/evals with escrowed funds and delivery criteria. This might mean some collectives STOP pushing to the public repo.\nv4: Policy hooks: standardized terms that institutions can adopt; preference signals integrated in provider contracts.\nv5: Federation: multiple flywheels interoperate on releases and preferences, raising both market depth and strike credibility.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flywheels and Markets</span>"
    ]
  },
  {
    "objectID": "04_options_for_flywheel.html",
    "href": "04_options_for_flywheel.html",
    "title": "5  Rationale (and other options for flywheel variants)",
    "section": "",
    "text": "5.1 Purpose of this section\nThis section gives more context about th many ways we might built flywheels, and lays out alternative governance paths and a future work (in particular, a focus on futures that involve healthy data markets, data intermediaries, federated learning, etc.)\nWe also discuss why we think an approach that includes a minimal retention frontend + opt-in flyhweel platform can serve as a pragmatic bridge to more advanced approaches. For instance, we can use the patterns and concepts used here to move towards independently governed data co-ops, eventual **federated learning, etc.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rationale (and other options for flywheel variants)</span>"
    ]
  },
  {
    "objectID": "04_options_for_flywheel.html#more-on-all-the-other-approaches-we-couldve-taken",
    "href": "04_options_for_flywheel.html#more-on-all-the-other-approaches-we-couldve-taken",
    "title": "5  Rationale (and other options for flywheel variants)",
    "section": "5.2 More on all the other approaches we could’ve taken",
    "text": "5.2 More on all the other approaches we could’ve taken\nIn choosing our architecture, we consider the following questions:\n\n5.2.1 Where does the “final” data live?\n\nCentralized Database: Traditional server-controlled storage (PostgreSQL, MongoDB, etc.)\nPublic Repository: Version-controlled platforms (GitHub, GitLab, Hugging Face Hub)\nTotally Local: Federated model where data stays on user devices\nOther options\n\nDistributed Network: Peer-to-peer systems (IPFS, BitTorrent)\nSomething hybrid?: Metadata centralized, actual data distributed\n\n\n\n\n5.2.2 When is the user prompted to contribute?\n\nProactive: User initiates contribution unprompted (e.g., “Share this chat” button)\nReactive: System prompts based on signals (e.g., after thumbs down or trigger word, ask “What went wrong?”)\nPassive: Automatic collection with prior consent (e.g., telemetry, browser extension)\nScheduled: Regular prompts (e.g., weekly “best conversations” review)\nTask-Based: Specific requests for data types (e.g., “Help us improve math responses”)\n\n\n\n5.2.3 What information object is created?\n\nSimple Signal: Binary feedback (👍/👎), star ratings, or flags\nAnnotated Conversation: Full chat with user corrections, ratings, or notes\nPreference Pair: A/B comparisons between responses\nExamples: User-created prompts and ideal responses\nStructured Feedback: Form-based input (error type, severity, correction)\nMultimodal Bundle: Text + images + voice + metadata\nMore advanced structured data …\n\n\n\n5.2.4 When is the data processed?\n\nPre-submission: Client-side processing before data leaves user’s device\nOn-submission: Real-time processing during the contribution flow\nPost-submission: Batch processing after data is received\nPre-publication: Review and processing before making data public\nOn-demand: Processing happens when data is accessed/downloaded\n\n(In practice, there may be some processing at various steps, but it is important to clarify this to users)\n\n\n5.2.5 How is the data accessed?\n\nDirect Download: Raw access to complete dataset (with rate limits)\nAPI Access: Programmatic access with authentication and quotas\nStatic Site: Read-only web interface with anti-scraping measures\nGated Access: Application/approval process for researchers\nHybrid Access: Public samples + gated full access, or public metadata + restricted content\nStreaming Access: Real-time feeds for continuous model training\n\n\n\n5.2.6 How much friction is acceptable?\n\nZero-Friction: One-click actions with no interruption\nLow-Friction: Modal popup or inline form\nMedium-Friction: Redirect to separate interface\nHigh-Friction: Multi-step process, account creation, or technical skills required",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rationale (and other options for flywheel variants)</span>"
    ]
  },
  {
    "objectID": "04_options_for_flywheel.html#some-categories-of-architectural-models",
    "href": "04_options_for_flywheel.html#some-categories-of-architectural-models",
    "title": "5  Rationale (and other options for flywheel variants)",
    "section": "5.3 Some Categories of Architectural Models",
    "text": "5.3 Some Categories of Architectural Models\n\n5.3.1 Standard “PrivateCo” Web App\nAn obvious option is to simply build a hosted “standard” “PrivateCo” /start-up style web app. In fact, in some contexts it may make sense to skip building an opt-in flyhweel and simply use the data generated by users directly for training, eval, etc. While one could argue that the Terms of Service for many existing tech products do make these products “opt in” in some sense, there are also serious downsides to the status quo (see e.g. Fiesler, Lampe, and Bruckman 2016.)\nWhile perhaps some users might prefer even prefer a start-up style model, we believe this would not be a good starting place for a public AI interface. We also believe it’s important to communicate to users how the public AI interface differs from e.g. using ChatGPT, Gemini, or AI overviews via search.\nThis approach doesn’t constrain how we answer the above questions, although it’s highly likely that data lives in centralized database and company can use telemetry/surveillance and keep friction low.\n\nExample Stack: Django/Rails + PostgreSQL, Next.js + MongoDB\n\nIf we use a PrivateCo model with focus on telemetry, our flywheel might look like:\n\n5.3.1.1 Direct Telemetry\n\nWhere data lives: Centralized analytics database\nWhen prompted: Passive (continuous collection)\nInformation object: Simple signals with context IDs\nWhen processed: On-submission (real-time pipeline)\nHow accessed: Aggregated dashboards only (no raw access)\nFriction level: Zero\nPros: Massive scale, unbiased sampling, real-time insights\nCons: Limited richness, privacy concerns, no corrections\nExample Stack: ClickHouse/BigQuery + streaming pipeline\n\n\n\n\n5.3.2 Git/Wiki Platform\nAnother option to build a “very active flywheel” (that arguably stretches the definition because friction will be very high) is to just deploy a server for git or wiki style peer production.\nNow, we likely constrain our answers to the above questions:\n\nWhere data lives: Public repository\nWhen prompted: Proactive (user initiates)\nInformation object: Markdown-formatted conversations\nWhen processed: Pre-submission (user does it) + CI/CD validation\nHow accessed: Direct download via Git + web interface\nFriction level: High (technical knowledge required)\nPros: Maximum transparency, built-in versioning, low cost\nCons: Excludes non-technical users, limited data types\nExample Stack: some combo of MediaWiki, GitHub, GitLab, HuggingFace + CI/CD validation\n\n\n\n5.3.3 Serverless + Git Platform\n\nWhere data lives: Public repository\nWhen prompted: Proactive or reactive\nInformation object: Structured data files (JSON/YAML)\nWhen processed: On-submission via serverless function\nHow accessed: Git access + static site generation\nFriction level: Low (automated complexity)\nPros: Transparency + usability, serverless scaling\nCons: Cold starts, API rate limits, complex error handling\nExample Stack: Vercel/Netlify + GitHub API + Hugging Face Hub\n\n\n\n5.3.4 Federated Learning Model\nOne radically different approach might involve using federated learning.\n\nWhere data lives: User devices (distributed)\nWhen prompted: Passive with consent\nInformation object: Model gradients or aggregated statistics\nWhen processed: Pre-submission (on-device)\nHow accessed: Only aggregated model updates available\nFriction level: Zero after setup\nPros: Maximum privacy, no data transfer, infinite scale\nCons: Complex implementation, limited debugging, device requirements\nExample Stack: Flower/TFF + edge deployment\n\n\n\n5.3.5 Browser Extension\nWe could implement a flywheel that relies on users downloading a browser extension! This only reflects a data ingestion choice: can be used with various backend choices above.\n\nWhere data lives: Centralized or distributed\nWhen prompted: Proactive or passive\nInformation object: DOM captures, interaction logs, selections\nWhen processed: Depends on backend\nHow accessed: Depends on storage choice\nFriction level: Very low after installation\n\n\n\n5.3.6 P2P Network Model\n\nWhere data lives: Distributed across peer nodes\nWhen prompted: Passive (background sharing)\nInformation object: Torrent-style data chunks\nWhen processed: Pre-submission by contributor + network validation\nHow accessed: P2P client required for full access\nFriction level: Medium (client installation)\nPros: No infrastructure costs, censorship resistant\nCons: Availability issues, complex coordination\nExample Stack: libp2p + BitTorrent protocol + DHT",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rationale (and other options for flywheel variants)</span>"
    ]
  },
  {
    "objectID": "04_options_for_flywheel.html#scenario-walkthroughs-a-practical-comparison",
    "href": "04_options_for_flywheel.html#scenario-walkthroughs-a-practical-comparison",
    "title": "5  Rationale (and other options for flywheel variants)",
    "section": "5.4 Scenario Walkthroughs: A Practical Comparison",
    "text": "5.4 Scenario Walkthroughs: A Practical Comparison\nHere, we walk through two common scenarios and describe what happens (in one sentence) for each of the architectures described above.\n#todo: these could be made crisper to highlight the key differences better (But also be honest about where there are similarities)\n\n5.4.1 Scenario A: User marks a chat as “Good” – when does processing happen?\n\nWeb App: Redirects to platform, PII scrubbed on submission, available via API after review\nGit/Wiki: User removes PII manually, creates PR, instantly visible on merge\nTelemetry: Signal sent, processed in real-time, only visible in aggregates\nHybrid: Signal sent immediately, full chat processed if shared\nServerless+Git: Modal appears, serverless function strips PII, PR created automatically\nFederated: Local processing only, contributes to next model update\nExtension: Captures state, removes PII client-side, sends to chosen backend\nP2P: Processes locally, shares with peers who validate before propagating\n\n\n\n5.4.2 Scenario B: User corrects a factual error\n\nWeb App: Editor interface, toxicity check on submission, published after human review\nGit/Wiki: User edits markdown, CI/CD checks format, visible immediately on merge\nTelemetry: Only captures “error” signal, no correction possible\nHybrid: Error signal triggers correction UI, correction queued for review\nServerless+Git: Inline correction, automated PII/toxicity checks, PR needs approval\nFederated: Correction processed locally, differential privacy applied\nExtension: Highlights error, pre-processes correction, sends to backend\nP2P: Broadcasts correction, network consensus before acceptance\n\n\n\n5.4.3 Scenario C: Accessing the contributed data\n\nWeb App: Researchers apply for API key, public sees samples on static site\nGit/Wiki: Anyone can clone repo, but rate-limited through CDN\nTelemetry: Only aggregated statistics available via public dashboard\nHybrid: Public can see signals dashboard, researchers apply for conversation access\nServerless+Git: Public (or gated) repo with all data, static site with search/filter\nFederated: No direct data access, only model checkpoints released\nExtension: Depends on backend choice, typically follows that model\nP2P: Must run client to access network, can specify data sharing preferences",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rationale (and other options for flywheel variants)</span>"
    ]
  },
  {
    "objectID": "04_options_for_flywheel.html#frontier-approaches-data-cooperatives-federated-learning-and-more",
    "href": "04_options_for_flywheel.html#frontier-approaches-data-cooperatives-federated-learning-and-more",
    "title": "5  Rationale (and other options for flywheel variants)",
    "section": "5.5 Frontier approaches data cooperatives, federated learning, and more",
    "text": "5.5 Frontier approaches data cooperatives, federated learning, and more\nIn many cases, users may want to have data governed by community organizations (e.g., organized by domain/region/language) that hold rights and decide release cadence, licensing defaults, and benefit policies.\nWe note that because our implementation is built on top of open-source software, communities can easily choose to deploy their own OpenWebUI instance and their own data flywheel and effectively operate entirely parallel, self-governed instances. If they also choose to share opt-in data via similar licensing and preference signal approaches, such datasets could be easily merged – but with fine-grained adjustments to precise details (e.g., slight modifications on retention, access, release cadence, content moderation, adn so on.) Of course, data co-ops may choose to use quite different technical stacks. This approach is just one among many.\nIt may be possible to also move from an opt-in data flywheel approach to a federated learning-first approach. Here, model training occurs across user or institutional nodes; only gradients/updates (with privacy tech) are centralized. The dataset remains partitioned or local; central custodian minimized. This approach would:\n\nReduces central data custody and breach surface\nAligns with data-residency and institutional constraints\nEnables “learning from data that can’t leave”\n\nBut has some major downsides / existing barriers:\n\nHarder reproducibility and data auditability\nComplex privacy stack (secure aggregation, DP, client attestation)\nBenchmarking must be redesigned (federated eval)\n\nThis is a bigger leap, but we believe it’s important to begin to think about how the implementation of the Public AI Data Flywheels might support communities wishing to transition towards an FL approach.\nOne rough sketch might look like: * Build the MVP defined in Chapter 2 * Ship license + AI-preference metadata (MVP). * Maintain gated HF releases and public leaderboards/full data access. * Publish provider-payload transparency and link to provider terms (no guarantees). * Process deletions via HF mechanisms when possible; keep our mirrors in sync. * Phase 1 — Co-op pilots * Charter one or two community co-ops; define bylaws, scope, and release cadence. * Spin up many instances of interface + flywheel combos (can fork software directly, or use similar approaches) * Establish a concrete sharing / merging plan * And beyond! * Once several independent data communities, are operated, it might be possible to move from lightweight sharing and merging to more serious federation with technical guarantees. Perhaps this might start with federated evaluation and then move to federated training. Much more to do here, out of scope for this document.",
    "crumbs": [
      "Intro",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rationale (and other options for flywheel variants)</span>"
    ]
  },
  {
    "objectID": "05_mvp.html",
    "href": "05_mvp.html",
    "title": "6  The Serverless + Git MVP",
    "section": "",
    "text": "6.1 Overview\nOur initial MVP of the flywheel is a “Serverless + Git Platform” approach. It is meant to be a robust and scalable starting point for the data flywheel. It strictly separates the “write” (contribution) and “read” (display) components of the system.\nThe overall goal is to enable opt-in contributions of data (prompt, output, good/bad, optional metadata) with relatively open licenses (per-item Creative Commons license: default is CC0, CC-BY, CC-BY-SA), state-of-the-art (caveat: also experimental / untested) preference signals (using IETF aipref draft spec + CC Preference Signals draft spec) and enforcement. The data is distributed via:\nFor full details, see the separate repo and its readme: #todo fill me in\nFor a short summary of the technical approach, see below bullet points:",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "05_mvp.html#overview",
    "href": "05_mvp.html#overview",
    "title": "6  The Serverless + Git MVP",
    "section": "",
    "text": "Hugging Face (gated) — full bundles by “license/usage bucket”; access via request.\nPublic site — leaderboards + full dataset access; Cloudflare WAF/bot controls to discourage bulk scraping.\n\n\n\n\nFrontend: A Next.js application hosted on Vercel, providing a simple, static interface for contributions.\n\nNo major lock-in here. Can easily be swapped for a lightweight static site, other modern web tech, etc.\n\nAuthentication: User identity is managed via Auth.js (Next-Auth), using Hugging Face (HF) as the exclusive OAuth provider. This allows for clear attribution of contributions to a user’s public HF username (if they choose to).\nData Storage: The single source of truth is a Hugging Face Dataset repository, which functions as a “Git-as-a-database.”\n\nStarting with HF as it is a platform with specific focus on AI datasets and open culture\n\nWaiting room approach: The implemented workflow follows a two-stage “waiting room” pattern to ensure data quality and safety:\nHow contribution works:\n\nA user logs in with their Hugging Face account.\nThe interface accepts contributions in three ways: by uploading an OpenWebUI JSON export file, via URL params, or by pasting plain text.\nThe frontend parser automatically detects the OpenWebUI format, extracts metadata (like model and tags), and prepends it as YAML frontmatter to the chat content.\nUsers choose a label for the chat type (“Good Chat” / “Fail Chat”) and attach (1) a Creative Commons license and (2) an IETF aipref and/or Creative Commons Preference Signal to signal preferences around AI use of the contribution.\nA pseudo-anonymity system allows users to contribute publicly with their HF username, as “anonymous,” or with a custom pseudonym.\nAn informed consent checkbox, linked to Terms of Service and FAQ pages, is required for all submissions.\nUpon submission, a serverless function writes the contribution not to the final dataset, but as a new, single JSON file in a _waiting_room/ directory within the Hugging Face repository. This operation is fast and avoids write conflicts.\n\nHow processing of the “waiting room” works:\n\nA separate, asynchronous script is run on a regular schedule (e.g., as a daily GitHub Action).\nThis script fetches all pending files from the _waiting_room.\nIt validates each contribution and includes a placeholder for future content moderation and PII checks.\nValidated contributions are batched and appended to a final, organized dataset file (e.g., data/2025-08.jsonl). Contributions are further bucketed by license/prefs.\nTo ensure atomicity, all file additions (to the final dataset) and deletions (from the waiting room) are performed in a single commit to the Hugging Face Hub. #todo when scaling, need to consider race conditions around the processing!",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "05_mvp.html#advantages-of-a-serverless-git-approach",
    "href": "05_mvp.html#advantages-of-a-serverless-git-approach",
    "title": "6  The Serverless + Git MVP",
    "section": "6.2 Advantages of a Serverless + Git approach",
    "text": "6.2 Advantages of a Serverless + Git approach\nA serverless + Git stack keeps the “write path” lightweight for contributors and cheap to operate. Functions spin up on demand and idle to zero, so we can avoid paying for boxes that sit around; the trade-off is cold starts, which are well-documented and can be mitigated with provisioned concurrency when needed.\nOn the “read path,” a static site on a global CDN gives instant distribution and low operational overhead. Pages (e.g., from Cloudflare, something like GitHub Pages, or similar) can read directly from the Git “source of truth” and serve assets from edge locations by default, which is exactly what we want for a browsable leaderboard and dataset browser.\nAdditionally, using the Hub (Git-backed) as the source of truth buys us a public audit trail and first-class versioning semantics. HF’s dataset repos are literally Git + LFS, with revision pinning via commit/tag/branch; storage is backed by object storage and scales. That maps cleanly to our “waiting room → release buckets” workflow and makes it easy to diff changes over time. (relevant HF docs: datasets, storage)\nModeration and PII handling are naturally centralized in the processing step. Because we trigger the write as a small file into a staging path and move it during a scheduled job, we can run filters, de-dup, and attach license/pref metadata before publication without asking contributors to learn tooling.\nBasic safety and access controls are pragmatic at the edge. Cloudflare’s WAF/Bot Management and newer “AI bot” controls give us a reasonable anti-scraping posture for public downloads and pages, even if nothing on the open web is truly copy-proof. Recent product updates explicitly target AI crawlers, with default blocking and challenge flows we can enable. (Cloudflare Docs, WIRED, Business Insider)\nFinally, the “preferences and licenses” story fits the stack. Dataset cards and metadata natively expose a license field and other tags (Hub UI and YAML), and CC licenses give clear obligations (e.g., BY, BY-SA) we can enforce in packaging and docs. That lets us partition releases by license and publish compatibility notes in a way downstream users can actually follow. (Hugging Face, Hugging Face)",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "05_mvp.html#disadvantages-and-what-to-watch",
    "href": "05_mvp.html#disadvantages-and-what-to-watch",
    "title": "6  The Serverless + Git MVP",
    "section": "6.3 Disadvantages (and what to watch)",
    "text": "6.3 Disadvantages (and what to watch)\nTrust centralizes in the processor. Contributors have to believe the middle layer won’t silently drop or reshape submissions. If/when we introduce event-driven ingestion (queues/streams), we must design for retries and duplicates.\nUX isn’t perfectly “instant.” There’s an inherent gap between a user pressing “share” and seeing their item on the public site, because we run validation and batching. That’s a conscious choice, but we should set expectations and likely provide some kind status expectations.\nOperationally, serverless isn’t “no ops,” it’s “different ops.” Cold starts exist (especially on sporadic paths), API limits are real on the platforms we hit (GitHub has documented ceilings; HF also rate-limits writes/reads even if specifics vary by endpoint), and “pay per use” can surprise us at scale without cost guardrails (see e.g. GitHub Docs, GitHub Docs).\nThere’s also platform coupling to be consider. Using specific hosted CI/CD, serverless runtimes, and hub APIs creates a degree of vendor lock-in; this is a known trade-off in serverless architectures and something we can blunt with portable formats (JSONL), documented exports, and “boring” interfaces (Git). ([CNCF][17])\nCompliance remains non-zero. Deletions across mirrored artifacts (Hub revisions, static site snapshots, downstream forks) take a clear policy and a repeatable playbook. For licenses, we’re responsible for honoring CC obligations in our packaging and comms (e.g., keeping BY attribution fields intact; not mixing BY-SA content into incompatible bundles). CC’s legalcode and guidance make those obligations explicit; our tooling should, too. (Creative Commons)\nNet: Serverless + Git gives us a pragmatic bridge—fast contributor UX, public versioning, cheap distribution—while we invest in moderation, idempotent processors, and clear license lanes. If we communicate the review gap, publish the processor’s rules, and keep escape hatches (export scripts, mirrors), the trade-offs are acceptable for an MVP and refine-able over time.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "05_mvp.html#other-reading",
    "href": "05_mvp.html#other-reading",
    "title": "6  The Serverless + Git MVP",
    "section": "6.4 Other reading:",
    "text": "6.4 Other reading:\n\nhttps://arxiv.org/abs/2109.02846\nhttps://datascience.codata.org/articles/10.5334/dsj-2021-012",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Serverless + Git MVP</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html",
    "href": "06_data_policy.html",
    "title": "7  Data Flow and Data Retention",
    "section": "",
    "text": "7.1 Glossary of Defined Terms (for this Chapter)\n“{The Public AI Chat Frontend}” or “Frontend” means the hosted interface described in this document that allows users to issue prompts to third-party model endpoints.\n“Open WebUI Instance” or “OWUI” means the hosted Open WebUI application at {{app_link}} (or successor URLs) that provides optional accounts and chat history.\n“Opt-in Data Flywheel” or “Flywheel” means the separate contribution and distribution platform at https://optinflywheel.com (or successor URLs) through which users may opt in to contribute data for public evaluation and research use.\n“Provider(s)” or “Model Endpoint(s)” means third-party model services (e.g., national labs, commercial providers) that receive user prompts and return model outputs. The Frontend is a gateway to these services and does not control their retention, training, or use practices.\n“Model Gateway” means the service layer that forwards requests from the Frontend to Providers and records a Request Envelope (metadata such as request ID, model ID, token counts, latency, and status).\n“Request Envelope” means non-content request metadata retained for reliability, capacity, and SLO monitoring.\n“Session Telemetry” means minimal first-party analytics collected on page load/navigation (e.g., timestamp, pseudonymous session ID, coarse locale, feature flags).\n“Security Logs” means IP address and User-Agent records used for rate-limiting and abuse detection.\n“Chat Object” means prompt(s), tool calls (if any), and model output(s) associated with a session or account within the Open WebUI Instance.\n“Contribution” means any data a user intentionally submits to the Flywheel (e.g., prompt/output pairs, tags, corrections), along with per-item License and AI Preference Signal selections.\n“AI Preference Signal” means an AI-use preference value the contributor attaches to a Contribution (e.g., IETF AI Preferences draft values and/or Creative Commons preference signals), intended to be conveyed downstream.\n“License” means the Creative Commons license selected by the contributor for a Contribution (supported in the MVP: CC0-1.0, CC-BY-4.0, CC-BY-SA-4.0).\n“License Bucket(s)” means the partitioning of Contributions into separate release artifacts by License (e.g., v1.0-cc0, v1.0-cc-by, v1.0-cc-by-sa).\n“Waiting Room” means the Flywheel’s gated staging directory (e.g., _waiting_room/ in a Hugging Face repository) where Contributions are first written prior to validation and release.\n“Release” means a published version of the dataset (and associated notes/checksums) assembled from validated Contributions, partitioned by License.\n“Gated Repository” means the Hugging Face dataset repository that requires an access request and acceptance of dataset-specific terms.\n“Static Site” means the public site that hosts leaderboards and provides full dataset access, with anti-scraping controls.\n“Anonymized Contributor ID” or “Pseudonym” means a non-identifying handle published with a Contribution when a contributor elects anonymity or a pseudonym instead of a Hugging Face username.\n“Personal Data” means information that identifies or can reasonably be linked to an individual; “Sensitive Personal Data” means Personal Data that is sensitive by law or policy (e.g., health, financial, precise location, government identifiers).\n“We/Us/Our” means [ENTITY NAME], the operator of the Frontend, the Open WebUI Instance, and the Flywheel.\n“You/Your” means the individual using the Frontend and/or contributing to the Flywheel, or the entity on whose behalf the individual acts.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#what-data-is-produced-when",
    "href": "06_data_policy.html#what-data-is-produced-when",
    "title": "7  Data Flow and Data Retention",
    "section": "7.2 What data is produced & when",
    "text": "7.2 What data is produced & when\n\n7.2.1 Open WebUI (no account required, but optional and recommended)\nYour OpenWebUI account and all associated data are stored and managed entirely by your OpenWebUI instance. In the case of the public AI MVP, both the flyhweel and frontend will be managed by the same organization, but in theory you could use the flywheel using an entirely separate OWUI instance (a community instance, your own, etc.)\nThe Flywheel App has no access to or control over the data stored within OpenWebUI. This includes:\n\nYour OpenWebUI Account: Your login credentials, email, and user settings.\nYour Full Chat History: All conversations you have within OpenWebUI are stored on that platform’s server.\nServer Logs & Analytics: Any logs or analytics generated by the OpenWebUI platform itself.\n\nTo understand how OpenWebUI collects, uses, and retains your data, you must consult the specific Terms of Service and Privacy Policy provided by the administrator of your OpenWebUI instance.\nThis data includes:\n\nSession telemetry (minimal) #todo: Double check telemetry and provide a sample paylod to show interested users;\n\nProduced when: Page load / navigation\nIncludes: Timestamp, pseudonymous session ID, coarse locale, feature flags\nStored in: First-party analytics\nAccess: Eng/Analytics (aggregated)\nDefault retention: Raw 30 days; aggregates 13 months\n\nIP & User-Agent (security) #todo double check this\n\nProduced when: Each request\nIncludes: IP, User-Agent for rate-limit/abuse detection\nStored in: Security log store\nAccess: SRE/Security\nDefault retention: 7 days, then delete/aggregate\n\nQuery content\n\nProduced when: On submit\nIncludes: Prompt + output\nStored in: interface database\nAccess: server admin only\nDefault retention: user can delete at any time; deleted along after 30 days of user inactivity. #todo, discuss this\n\nError logs (sanitized) #todo double check this, provide example? least important to provide an example here.\n\nProduced when: On error\nIncludes: Stack trace, request ID (no prompt/output bodies)\nStored in: Log store\nAccess: Eng (least privilege)\nDefault retention: 30 days\n\nProfile and settings #todo double check this\n\nProduced when: Sign-up\nIncludes: Email/OAuth ID, display name\nStored in: User DB\nAccess: Support/Eng\nRetention: Kept until the user deletes it. If the account is inactive for 30 days, we delete the account and associated records.\n\n\n\n\n7.2.2 Data Sent from OpenWebUI to Model Gateway (to providers)\n\nRequest envelope\n\nProduced when: Each request\nIncludes: Request ID, model ID, token counts, latency, status\nStored in: Metrics DB\nAccess: Eng/SRE\nRetention: 90 days (aggregates 13 months)\n\n\n\nProvider transparency: We do not guarantee provider behavior. We clearly display the exact payload we forward (headers + body summary) and link to the provider’s own terms and policies where available. Users should review provider terms before use.\n\n\n\n7.2.3 Data Sent From OpenWebUI to the Flywheel\nWithin the OpenWebUI interface, you may have the option to explicitly opt-in and share a specific chat with this public dataset project. When you choose to do this, OpenWebUI sends a copy of that single chat to our /api/contributions/ingest endpoint.\nOnce that data is received by our application, it is handled according to the policies described in this document. The data packet we receive includes:\n\nThe chat content and metadata you selected.\nYour OpenWebUI username (for display purposes if you choose “public”).\nA unique, non-identifying provider_user_id (e.g., user-123). We use this to create a contributor_hash to group your anonymous contributions, but we never store the raw ID itself in the final dataset.\n\nThis looks like:\n\nContribution payload\n\nProduced when: On explicit opt-in\nIncludes: Prompt, model output, optional tags\nStored in: Gated staging (effectively public) → license-bucketed release (also effectively public)\nAccess: Public\nRetention: Indefinite\n\nHuggingFace username OR Anonymized contributor ID\n\nProduced when: On ingest\nIncludes: user provides their huggingface username or selects a pseudonym (can leave blank)\nStored in: Dataset metadata\nAccess: Public\nRetention: Indefinite\n\nRelease artifacts\n\nProduced when: On release\nIncludes: JSONL/TSV, checksums, notes\nStored in: HF (gated) and Static Site (public)\nAccess: Public (per channel)\nRetention: Indefinite\n\n\nWhen you sign in directly to this Contribution App’s web interface, you are authenticated through your Hugging Face account using the standard OAuth protocol.\n\nWhat we use: We request the openid, profile, and email scopes from Hugging Face. The application uses your public Hugging Face username (also called a “handle”) to identify you. This username is stored in a secure, encrypted session cookie in your browser.\nWhat we don’t store: Your email address and Hugging Face password are never stored or logged by our application. The session cookie is deleted when you sign out or it expires.\nWho can access it: Only the application’s backend can read your session cookie to verify you are logged in when you submit a contribution.\nRetention: This data is ephemeral and only lasts for the duration of your active session.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#server-api-data",
    "href": "06_data_policy.html#server-api-data",
    "title": "7  Data Flow and Data Retention",
    "section": "7.3 Server & API Data",
    "text": "7.3 Server & API Data\nTo ensure security, stability, and prevent abuse of the Contribution App, we handle technical data related to your requests.\n\n7.3.1 Rate Limiting\nTo prevent spam and ensure the service is available for everyone, we limit the number of requests a single user can make.\n\nWhat we use: The middleware.ts file shows that we use your IP address to enforce a rate limit (e.g., 100 requests per hour). For ingestions from OpenWebUI, we rate-limit based on their API key.\nWho can access it: Your IP address is sent to Upstash Redis, our rate-limiting service provider.\nRetention: Upstash retains a record of your IP address for the duration of the rate-limiting window, which is 1 hour.\n\n\n\n7.3.2 Server Logs\nLike most web applications, our hosting platform (Vercel) automatically generates server logs for every request.\n\nWhat they contain: These logs may include your IP address, user-agent string (browser information), the requested URL, response status code, and other standard request headers. If an error occurs, the log might contain details about the error to help us debug.\nWho can access it: Project Maintainers can access these logs through the Vercel dashboard for debugging and monitoring purposes.\nRetention: Log retention is determined by Vercel’s platform policies, which is typically between 1 and 30 days.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#event-timeline-how-data-flows",
    "href": "06_data_policy.html#event-timeline-how-data-flows",
    "title": "7  Data Flow and Data Retention",
    "section": "7.4 Event timeline (how data flows)",
    "text": "7.4 Event timeline (how data flows)\n\nUser prompts a model → payload sent to model provider; query and response are saved if user made an optional OpenWebUI account.\nIf user wishes to select a chat to share via opt-in, they can do so. They never have to. Licensing and preference signals are set at opt-in time.\nOpt-in chat goes to “waiting room”\nProcessing script collects items from waiting room, applies some checks (PII, content moderation), moves them into release buckets.\nA separate processing script takes data from the gated HF repo and builds the static site with leaderboard and data dump\nPost-release: approved removals are honored in future releases and our mirrors; prior downloads may persist.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#more-on-the-flywheel",
    "href": "06_data_policy.html#more-on-the-flywheel",
    "title": "7  Data Flow and Data Retention",
    "section": "7.5 More on the flywheel",
    "text": "7.5 More on the flywheel\nThis is the core data lifecycle for every chat you contribute. Your submission goes through several automated stages before it becomes a permanent part of the public dataset.\n\n7.5.1 Phase 1: Submission\nWhether you use the web form or contribute via OpenWebUI, you provide the data for a single conversation.\n\n\n7.5.2 Phase 2: The Waiting Room (Temporary)\nImmediately after submission, your contribution is packaged into a single JSON file and uploaded to a private _waiting_room directory in our Hugging Face dataset repository.\n\nWhat’s in the file: This file contains the raw content and metadata from your submission. To uniquely and privately identify contributors, we generate a contributor_hash:\n\nFor web submissions, this is a SHA256 hash of your Hugging Face username combined with a secret salt: sha256(salt + hf_username).\nFor OWUI submissions, this is a hash of the provider and your provider user ID: sha256(salt + \"openwebui:user-123\").\n\nWho can access it: Only Project Maintainers with access to the repository can see these files.\nRetention: These files are temporary. They exist only until the processing script runs, typically within a few minutes or hours, after which they are deleted.\n\n\n\n7.5.3 Phase 3: Processing & PII Redaction\nA script (process_pending.ts) periodically runs to process every file in the waiting room. Its primary job is to validate the data and automatically redact Personally Identifiable Information (PII) from the chat content. Our redaction script looks for and replaces the following patterns:\n\nEmail addresses\nIP addresses (IPv4 & IPv6)\nSocial Security Numbers (SSN)\nIBAN bank account numbers\nEthereum and Bitcoin wallet addresses\nPhone numbers\nCredit card numbers (verified with Luhn check)\nSimple name patterns (e.g., “my name is John Doe”)\n\nIf more than 5 potential PII hits are found, the file is moved to quarantine for safety.\n\n\n7.5.4 Phase 4: The Quarantine Zone\nIf a submission fails processing, it’s moved to a private _quarantined directory.\n\nWhy it’s quarantined: A file is quarantined if it is malformed (e.g., invalid JSON), fails our content validation rules, or has too many PII hits detected by the redaction script.\nWho can access it: Only Project Maintainers can access these files to diagnose processing errors.\nRetention: Quarantined files are kept indefinitely for manual review.\n\n\n\n7.5.5 Phase 5: The Final Public Dataset (Permanent) ✅\nAfter successful processing and PII redaction, your contribution is appended to a monthly JSON Lines file (e.g., data/2025-08.jsonl).\n\nWhat it is: This is the final, permanent, and clean version of your contribution. The content has been redacted, and the metadata is structured according to our public schema.\nWho can access it: This dataset is public. Anyone in the world can view, download, and use it according to the license you chose for your contribution.\nRetention: Contributions to this dataset are perpetual and irrevocable, as stated in the Terms of Service you agree to upon submission.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#licensing-preference-signals-beta",
    "href": "06_data_policy.html#licensing-preference-signals-beta",
    "title": "7  Data Flow and Data Retention",
    "section": "7.6 Licensing & Preference Signals (Beta)",
    "text": "7.6 Licensing & Preference Signals (Beta)\nFor each contribution, the user selects:\n\nA Creative Commons license: CC0-1.0, CC-BY-4.0, or CC-BY-SA-4.0.\nAn AI preference signal: an IETF AI preference (draft) value and/or CC preference signal (“IETF AI Pref combo”).\n\nHow we use these:\n\nWe record license and ai_pref per record.\nRecords are partitioned by license into separate release buckets.\nDownstream users must comply with the license; we publish compatibility notes (e.g., ShareAlike).\nUsers may set account defaults; each submission can override.\n\nBinding effect: Once a record is included in a release, that license applies to that copy.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#example-retention-schedule",
    "href": "06_data_policy.html#example-retention-schedule",
    "title": "7  Data Flow and Data Retention",
    "section": "7.7 Example Retention schedule",
    "text": "7.7 Example Retention schedule\n\nWeb edge\n\nData: IP + UA\nPurpose: Abuse prevention\nRetention: 7 days raw → delete/aggregate\nDeletion path: Automatic purge\n\nWeb UI\n\nData: Minimal telemetry\nPurpose: Reliability metrics\nRetention: 7 days raw → delete/aggregate\nDeletion path: Automatic purge\n\nGateway\n\nData: Request envelopes\nPurpose: Capacity/SLOs\nRetention: 90 days raw; 13 months aggregates\nDeletion path: Automatic purge\n\nOpen WebUI Accounts\n\nData: Profile and preferences\nPurpose: Auth/consent\nRetention: Kept until user deletes; 30-day inactivity → delete\nDeletion path: Self-service delete / auto-purge\n\nFlywheel staging bucket (the “waiting room dir”, on HF)\n\nData: Pending contributions\nPurpose: Moderation/de-duplication\nRetention: Indefinite (moved to organized buckets in same repo)\nDeletion path: Manual removal on request\n\nHF (gated)\n\nData: Released records (by license)\nPurpose: Research access\nRetention: Indefinite\nDeletion path: Exclude from future versions; coordinate HF deletion where possible\n\nStatic Site (public)\n\nData: Leaderboards and full dataset access\nPurpose: Benchmarking/transparency\nRetention: Indefinite\nDeletion path: Update/remove in future releases; past d",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#distribution-access-control",
    "href": "06_data_policy.html#distribution-access-control",
    "title": "7  Data Flow and Data Retention",
    "section": "7.8 Distribution & access control",
    "text": "7.8 Distribution & access control\n\n7.8.1 Hugging Face (gated)\n\nSeparate artifacts and checksums per license bucket and version (e.g., v1.0-cc0, v1.0-cc-by, v1.0-cc-by-sa).\nAccess requires a request with acceptance of dataset-specific Terms of Use (no re-identification; honor license and AI preferences).\nDeletion requests: Where possible, we use Hugging Face–native workflows (e.g., issue/repo requests, maintainers’ takedowns) to process deletions and exclude items from future versions.\n\n\n\n7.8.2 Flywheel Static Site (public)\n\nHosts leaderboards and provides full dataset access for building benchmarks.\nCloudflare anti-scraping posture: Bot Management/WAF rules, rate limits, Turnstile/JS challenges on download routes, tokenized short-lived URLs, robots/meta controls, pagination/throttling, and anomaly monitoring.\nReality check: Anti-scraping reduces but cannot prevent copying of public data. We limit risk via license partitioning, logged access flows, and clear terms.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#contributor-rights-controls",
    "href": "06_data_policy.html#contributor-rights-controls",
    "title": "7  Data Flow and Data Retention",
    "section": "7.9 Contributor rights & controls",
    "text": "7.9 Contributor rights & controls\n\nOpt-in only for contributions to the flywheel.\nPer-item license and AI preference (beta).\nWithdrawals: Submit a verified request to exclude your items from future releases. We’ll also file/route requests through Hugging Face where applicable.\nAccount deletion & inactivity: If you keep an Open WebUI account, we keep your account data until you delete it; if you are inactive for 30 days, we delete the account and associated records. Released dataset copies remain under their chosen licenses.\nSensitive content: Do not submit personal/sensitive data; automated filters and moderation apply.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#roadmap",
    "href": "06_data_policy.html#roadmap",
    "title": "7  Data Flow and Data Retention",
    "section": "7.10 9) Roadmap",
    "text": "7.10 9) Roadmap\n\nHF-auth contributions: Let contributors submit directly via their Hugging Face accounts to preserve reputation and contributor stats.\nAI preference UX: First-class UI for selecting license + IETF AI Pref combo, plus validator and compatibility helper.\nProvider term links directory: Central index of provider policies; per-model tooltips in the WebUI.",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "06_data_policy.html#contacts",
    "href": "06_data_policy.html#contacts",
    "title": "7  Data Flow and Data Retention",
    "section": "7.11 Contacts",
    "text": "7.11 Contacts\n#todo\n\nData removal / privacy: {{PRIVACY_EMAIL}}\nSecurity: {{SECURITY_EMAIL}}\nResearch & benchmarks: {{RESEARCH_EMAIL}}",
    "crumbs": [
      "A Worked Example: MVP Flywheel",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Flow and Data Retention</span>"
    ]
  },
  {
    "objectID": "appendix1_llm_data.html",
    "href": "appendix1_llm_data.html",
    "title": "8  Appendix 1: LLM Data Schemas",
    "section": "",
    "text": "Here, we describe many variants of LLM data. This will be relevant for when we extend the flywheel to include more types of data, and especially shift towards promoting the sharing (via opt-in flywheels, but also via new market mechanisms) of richer “content data”.\n\nOpen Web / Crawls\n\nWARC/WAT/WET\n\nWARC (container for HTTP request/response records) — spec & overview: IIPC WARC 1.1; Library of Congress format note. (IIPC Community Resources, The Library of Congress)\nWAT (JSON metadata extracted from WARC) and WET (plain text extracted from HTML) — Common Crawl guides. (Common Crawl, Common Crawl)\n\nC4 (Colossal Clean Crawled Corpus) — TFDS catalog & generator code. Fields are essentially clean text segments with basic metadata. (TensorFlow, GitHub)\nThe Pile (22-source, mixed corpus) — paper & HTML view. (arXiv, ar5iv)\n\nEncyclopedic / Books\n\nWikipedia XML dumps (page/revision XML; SQL tables for links) — Meta-Wiki dump format; Wikipedia database download. (Meta, Wikipedia)\nProject Gutenberg\n\nBooks: plain text/HTML master formats; ePub/MOBI derived. (Project Gutenberg)\nCatalog schema: daily RDF/XML (also CSV) for metadata; offline catalogs. (Project Gutenberg)\n\n\nScientific / Legal\n\narXiv (Atom/OAI-PMH metadata; bulk & API) — OAI-PMH + API docs; bulk metadata page. (info.arxiv.org, info.arxiv.org, info.arxiv.org)\nJATS XML (journal article tag suite) — NISO standards; NLM JATS site. (niso.org, jats.nlm.nih.gov)\n\nCode\n\nBigCode — The Stack / The Stack v2 (source files + license/provenance metadata; dedup variants) — HF datasets, project docs, arXiv overview. (Hugging Face, Hugging Face, BigCode, arXiv)\n\nForums / Q&A / Social\n\nStack Exchange dumps (XML: Posts, Users, Comments, Votes, etc.) — SE Meta/docs & Data Explorer. (Meta Stack Exchange, data.stackexchange.com)\nReddit\n\nAPI JSON schema — official API docs & help. (Reddit, Reddit Help)\nPushshift (historical dumps; research dataset) — site & paper. (pushshift.io, arXiv)\n\n\nInstruction / Conversations (Post-training SFT)\n\nOpenAI-style chat schema (role-tagged: system|user|assistant, plus tool calls) — API reference. (OpenAI Platform)\nAlpaca (JSON prompts/instructions/outputs) — Stanford post & repo; cleaned community set. (crfm.stanford.edu, GitHub, GitHub)\nDatabricks Dolly-15k (human-written instruction/response pairs) — repo. (GitHub)\nOpenAssistant OASST1 (message-tree conversations with roles) — HF dataset card. (Hugging Face)\n\nPreference / Feedback (RLHF & DPO)\n\nHH-RLHF (Anthropic helpful/harmless, JSONL pairs: chosen vs rejected) — dataset repo readme. (GitHub)\nDPO format (prompt + preferred vs dispreferred response) — DPO paper. (arXiv)\n\nMultimodal (for VLMs/ASR)\n\nLAION-5B / Re-LAION-5B (image–text pairs with CLIP scores; links) — LAION posts. (laion.ai, laion.ai)\nWhisper (weakly-supervised ASR; audio → text pairs) — paper & blog. (arXiv, OpenAI)\nHowTo100M (YouTube instructional video clips + narrations) — project page & paper. (di.ens.fr, arXiv)\n\nMath-reasoning (often for post-training/eval)\n\nGSM8K (grade-school word problems; JSON) — repo & HF dataset card. (GitHub, Hugging Face)\nMATH (competition problems with step-by-step solutions) — paper & HF. (arXiv, Hugging Face)\n\nCommon storage containers\n\nJSON Lines / NDJSON — jsonlines.org; ndjson spec. (jsonlines.org, GitHub)\nTFRecord — TensorFlow tutorial. (TensorFlow)\nApache Parquet — project site. (Apache Parquet)\n\n\n#todo check all refs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Appendix 1: LLM Data Schemas</span>"
    ]
  },
  {
    "objectID": "appendix2_prefsig.html",
    "href": "appendix2_prefsig.html",
    "title": "9  Appendix 2 — Preference Signals for AI Data Use (CC signals + IETF AI Preferences)",
    "section": "",
    "text": "#todo: improve the references here to specific lines of IETF draft and the CC Preference Signals FAQ\n\nWhat CC signals are A Creative Commons framework for reciprocal AI reuse: content stewards can allow specific machine uses if certain conditions are met (e.g., credit, contributions, openness). Overview & implementation notes. (homepage, implementation)\nFour proposed CC signals (v0.1)\n\nCredit (cc-cr) — cite the dataset/collection; RAG-style outputs should link back when feasible.\nCredit + Direct Contribution (cc-cr-dc) — proportional financial/in-kind support.\nCredit + Ecosystem Contribution (cc-cr-ec) — contribute to broader commons.\nCredit + Open (cc-cr-op) — release model/code/data to keep the chain open. Source (draft repo & posts). (GitHub, Creative Commons)\n\nIETF AI Preferences (aipref) — the transport & vocabulary\n\nVocabulary: a machine-readable set of categories (e.g., ai-use, train-genai) and preferences (y = grant, n = deny) with exceptions. Drafts. (datatracker.ietf.org, IETF, IETF AI Preferences Working Group)\nAttachment: how to convey these preferences via HTTP Content-Usage header and robots.txt extensions. Drafts. (datatracker.ietf.org, IETF)\nStructured Fields: uses RFC-standardized HTTP structured field values. (datatracker.ietf.org, datatracker.ietf.org, rfc-editor.org)\nRobots Exclusion Protocol baseline. (datatracker.ietf.org, rfc-editor.org)\n\nPutting them together (content-usage expression)\n\nShape:\n&lt;category&gt;=&lt;y|n&gt;;exceptions=&lt;cc-signal&gt;\nExample in robots.txt (allow everything, but AI use denied unless Credit):\nUser-Agent: *\nContent-Usage: ai-use=n;exceptions=cc-cr\nAllow: /\nExample HTTP header (deny gen-AI training unless Credit + Ecosystem):\nContent-Usage: train-genai=n;exceptions=cc-cr-ec\n(Syntax and examples from CC & IETF drafts.) (Creative Commons, IETF)\n\nOperational notes (for this repo’s flywheel)\n\nPer-record fields to store: license (CC0/CC-BY/CC-BY-SA) and ai_pref (IETF aipref value + optional CC signal), plus optional attribution handle. (Aligns with CC write-ups & IETF drafts.) (Creative Commons, datatracker.ietf.org)\nPlacement:\n\nLocation-based signals via robots.txt for site/paths. (datatracker.ietf.org)\nUnit-based signals via HTTP Content-Usage on dataset files and API responses. (datatracker.ietf.org)\n\nInteroperability expectations: signals are normative preferences; adherence relies on ecosystem norms (similar to robots.txt & CC license culture). (Creative Commons)\n\nContext & momentum\n\nCC’s 2025 launch posts; IETF WG activity updates (e.g., IPTC note). (Creative Commons, Creative Commons, IPTC)\n\n\n\n#todo check all refs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Appendix 2 — Preference Signals for AI Data Use (CC signals + IETF AI Preferences)</span>"
    ]
  },
  {
    "objectID": "appendix3_example_legalterms.html",
    "href": "appendix3_example_legalterms.html",
    "title": "10  Appendix 3: Example Legal Terms",
    "section": "",
    "text": "10.1 Opt-in Data Flywheel — Legal Terms (Draft)\nEffective: [DATE]\nThrough the Opt-in Data Flywheel, you may contribute chats, corrections, and related materials to build openly accessible evaluation sets and datasets.\nYou may participate only if you agree to these Opt-in Data Flywheel Legal Terms (the “Terms”). 1. Eligibility\nThe Flywheel is open to individuals who are the age of majority in their jurisdiction, or to younger participants with verified parental/guardian consent and supervision. You must also comply with Our Community Guidelines/Acceptable Use Policy ([LINK]). 2. Your Contributions; Licensing; AI Preferences\n2.1 Opt-in Only. Submitting to the Flywheel is purely voluntary and separate from using the Open WebUI Instance.\n2.2 License Grant (per item). For each Contribution, you select one License (CC0-1.0, CC-BY-4.0, or CC-BY-SA-4.0). You grant Us a non-exclusive, worldwide right to publish, reproduce, modify (solely for formatting, moderation, and aggregation), distribute, and sublicense the Contribution under the selected License. Once included in a Release, that License applies to that copy of the Contribution.\n2.3 AI Preference Signals. If you attach an AI Preference Signal, We will transmit and display it with the Contribution and document how Our systems interpret such signals. We cannot guarantee that downstream users or Providers will honor such signals.\n2.4 Assurances. You represent and warrant that (a) you have the necessary rights to your Contributions; (b) your Contributions do not infringe third-party rights; (c) you will not include Sensitive Personal Data; and (d) you will comply with Our Acceptable Use Policy. 3. Accounts; Attribution; Pseudonymity\n3.1 Auth. Contributions require authentication (e.g., Hugging Face OAuth). 3.2 Attribution. You may choose to publish under your Hugging Face username, under a pseudonym, or as “anonymous.” 3.3 Leaderboards. We may publish contribution metrics (counts, languages, tags) with your chosen public handle. We will not publish your email address. 4. Processing; Waiting Room; Release\n4.1 Waiting Room. Submissions write to a staging directory. 4.2 Validation. We may run automated and human review for formatting, de-duplication, PII/safety checks, and License/AI-preference validation. 4.3 Release. Validated items are appended to License Buckets (e.g., vYYYY-MM) and published to a Gated Repository and mirrored to the Static Site. 5. Distribution; Access Control\n5.1 Gated Repository. Access requires acceptance of dataset-specific terms (e.g., no re-identification; respect License and AI preferences). 5.2 Static Site. Public access includes anti-scraping measures (WAF/bot management, rate limits, tokenized URLs). Copying cannot be fully prevented; rely on License controls for downstream obligations. 6. Deletions & Takedowns\n6.1 Future-Only Removal. Upon verified request, We will exclude the identified Contribution(s) from future Releases and update mirrors where feasible. Past Releases and third-party copies may persist. 6.2 Hugging Face Workflows. Where possible, We will route or honor takedowns via the Hugging Face repository’s native workflows. 7. Provider Transparency (No Guarantees)\nWe forward prompts to third-party Providers. We display a payload transparency panel and link to Provider terms when available. We do not control Provider retention, training, or other uses of data once sent to them. 8. Privacy; Retention\nRetention and access for telemetry, envelopes, accounts, staging, Releases, and the Static Site are governed by the Data Retention & Contribution Policy (Section 3). That Policy is incorporated by reference. 9. Communications\nBy creating an account or requesting repository access, you may receive administrative emails (e.g., access decisions, policy updates). 10. Disclaimers; Limitation of Liability; Indemnity\nTHE FLYWHEEL AND RELEASES ARE PROVIDED “AS IS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES (INCLUDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT). WE WILL NOT BE LIABLE FOR INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY, OR PUNITIVE DAMAGES. Our aggregate liability under these Terms will not exceed USD $500 (or the maximum permitted by law if lower). You agree to indemnify Us for third-party claims arising from your Contributions or breach of these Terms. 11. Updates\nWe may update these Terms by posting a new effective date. Continued use after the effective date constitutes acceptance. 12. Termination\nWe may suspend or terminate access at any time. Contributions included in prior Releases remain available under their Licenses. 13. Governing Law; Venue\nThese Terms are governed by the laws of [LAW & VENUE], without regard to conflict-of-laws rules. Exclusive venue lies in the courts of [VENUE].",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix 3: Example Legal Terms</span>"
    ]
  },
  {
    "objectID": "appendix3_example_legalterms.html#frontend-instance",
    "href": "appendix3_example_legalterms.html#frontend-instance",
    "title": "10  Appendix 3: Example Legal Terms",
    "section": "10.2 Frontend Instance",
    "text": "10.2 Frontend Instance\nOpen WebUI Instance — Terms of Use (Draft)\nEffective: [DATE]\nThese Terms govern your use of Our hosted Open WebUI Instance at {{app_link}} (or successor URLs). 1. Eligibility; Community Rules\nThe service is available to individuals who are the age of majority in their jurisdiction, or younger participants with verified parental/guardian consent and supervision. You must follow Our Community Guidelines/Acceptable Use Policy ([LINK]). 2. Accounts; Content\n2.1 Accounts Optional. You may use OWUI without an account; certain features (history, settings, opt-in share flows) require an account. 2.2 Your Content. Prompts and outputs in your account are stored as Chat Objects to provide history and UX features. They are not used for training or evaluation by Us unless you explicitly opt in via the Flywheel. 2.3 Feedback Data. Thumbs, flags, and similar signals may be stored to improve product reliability and moderation and are handled per Section 3 (Retention Policy). 3. Provider Transparency\nOWUI forwards your prompts to third-party Providers. We display a payload transparency panel and, where available, links to Provider terms. We do not control Provider retention, training, or other uses of your data. 4. Privacy; Retention; Security\nRetention, deletion, and access controls for telemetry, Security Logs, Request Envelopes, account data, and error logs are governed by the Data Retention & Contribution Policy (Section 3), incorporated here by reference. 5. Sharing to the Flywheel\nSharing to the Flywheel is separate and requires explicit opt-in with per-item License and AI Preference Signal selections. See the Flywheel Terms. 6. Acceptable Use\nYou agree not to: (a) upload Sensitive Personal Data; (b) violate laws or third-party rights; (c) attempt to reverse engineer or abuse rate limits; (d) circumvent access controls; or (e) interfere with service integrity. 7. Communications\nIf you create an account, We may send administrative emails (e.g., login links, security alerts, policy updates). 8. Disclaimers; Limitation of Liability\nOWUI IS PROVIDED “AS IS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES (INCLUDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT). WE WILL NOT BE LIABLE FOR INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY, OR PUNITIVE DAMAGES. Our aggregate liability will not exceed USD $500 (or the maximum permitted by law if lower). 9. Updates; Termination\nWe may update these Terms by posting a new effective date. Continued use after the effective date constitutes acceptance. We may suspend or terminate accounts for any reason, including AUP violations or security risk. 10. Governing Law; Venue\nThese Terms are governed by the laws of [LAW & VENUE], without regard to conflict-of-laws rules. Exclusive venue lies in the courts of [VENUE].",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix 3: Example Legal Terms</span>"
    ]
  }
]